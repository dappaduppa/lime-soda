<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 252 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_252{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_252{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_252{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_252{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
class="s1_252"
>Maximum Likelihood Estimation</text>

<text 
x="55" 
y="122" 
class="s2_252"
>Why choose least squares? One justification involves</text>

<text 
x="529" 
y="122" 
class="s3_252"
>maximum likelihood estimation</text>

<text 
x="805" 
y="122" 
class="s2_252"
>.</text>

<text 
x="55" 
y="160" 
class="s2_252"
>Imagine that we have a sample of data</text>

<image preserveAspectRatio="none" x="398" y="141" width="105" height="20" xlink:href="252/img/1.png" />
<text 
x="508" 
y="160" 
class="s2_252"
>that comes from a distribution that</text>

<text 
x="55" 
y="189" 
class="s2_252"
>depends on some unknown parameter</text>

<image preserveAspectRatio="none" x="393" y="167" width="16" height="22" xlink:href="252/img/2.png" />
<text 
x="409" 
y="189" 
class="s2_252"
>:</text>

<image preserveAspectRatio="none" x="55" y="207" width="428" height="68" xlink:href="252/img/3.png" />
<text 
x="55" 
y="306" 
dx="0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_252"
>If we didn’t know theta, we could turn around and think of this quantity as the</text>

<text 
x="747" 
y="306" 
class="s3_252"
>likelihood</text>

<text 
x="55" 
y="335" 
class="s2_252"
>of</text>

<image preserveAspectRatio="none" x="79" y="314" width="17" height="22" xlink:href="252/img/4.png" />
<text 
x="100" 
y="335" 
class="s2_252"
>given the sample:</text>

<image preserveAspectRatio="none" x="55" y="353" width="434" height="68" xlink:href="252/img/5.png" />
<text 
x="55" 
y="454" 
class="s2_252"
>Under this approach, the most likely</text>

<image preserveAspectRatio="none" x="380" y="432" width="17" height="22" xlink:href="252/img/6.png" />
<text 
x="402" 
y="454" 
class="s2_252"
>is the value that maximizes this likelihood</text>

<text 
x="55" 
y="481" 
class="s2_252"
>function; that is, the value that makes the observed data the most probable. In the case of a</text>

<text 
x="55" 
y="509" 
class="s2_252"
>continuous distribution, in which we have a probability distribution function rather than a</text>

<text 
x="55" 
y="536" 
class="s2_252"
>probability mass function, we can do the same thing.</text>

<text 
x="55" 
y="575" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_252"
>Back to regression. One assumption that’s often made about the simple regression model</text>

<text 
x="55" 
y="602" 
class="s2_252"
>is that the regression errors are normally distributed with mean 0 and some (known)</text>

<text 
x="55" 
y="630" 
class="s2_252"
>standard deviation</text>

<image preserveAspectRatio="none" x="222" y="616" width="17" height="14" xlink:href="252/img/7.png" />
<text 
x="238" 
y="630" 
dx="0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_252"
>. If that’s the case, then the likelihood based on seeing a pair</text>

<text 
x="774" 
y="630" 
class="s4_252"
>(x_i,</text>

<text 
x="55" 
y="660" 
class="s4_252"
>y_i)</text>

<text 
x="102" 
y="660" 
class="s2_252"
>is:</text>

<image preserveAspectRatio="none" x="55" y="680" width="799" height="64" xlink:href="252/img/8.png" />
<text 
x="55" 
y="775" 
class="s2_252"
>The likelihood based on the entire data set is the product of the individual likelihoods,</text>

<text 
x="55" 
y="803" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_252"
>which is largest precisely when</text>

<text 
x="336" 
y="803" 
class="s4_252"
>alpha</text>

<text 
x="394" 
y="803" 
class="s2_252"
>and</text>

<text 
x="431" 
y="803" 
class="s4_252"
>beta</text>

<text 
x="479" 
y="803" 
class="s2_252"
>are chosen to minimize the sum of</text>

<text 
x="55" 
y="832" 
class="s2_252"
>squared errors. That is, in this case (and with these assumptions), minimizing the sum of</text>

<text 
x="55" 
y="860" 
class="s2_252"
>squared errors is equivalent to maximizing the likelihood of the observed data.</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
