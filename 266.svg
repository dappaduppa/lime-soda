<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 266 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_266{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_266{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_266{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_266{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s5_266{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}
.s6_266{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s7_266{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC00FF;
}
.s8_266{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s9_266{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s10_266{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}
.s11_266{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #CC3300;
}
.s12_266{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
class="s1_266"
>Regularization</text>

<text 
x="55" 
y="122" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_266"
>In practice, you’d often like to apply linear regression to data sets with large numbers of</text>

<text 
x="55" 
y="149" 
class="s2_266"
>variables. This creates a couple of extra wrinkles. First, the more variables you use, the</text>

<text 
x="55" 
y="177" 
class="s2_266"
>more likely you are to overfit your model to the training set. And second, the more</text>

<text 
x="55" 
y="204" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_266"
>nonzero coefficients you have, the harder it is to make sense of them. If the goal is to</text>

<text 
x="55" 
y="232" 
class="s3_266"
>explain</text>

<text 
x="125" 
y="232" 
class="s2_266"
>some phenomenon, a sparse model with three factors might be more useful than a</text>

<text 
x="55" 
y="259" 
class="s2_266"
>slightly better model with hundreds.</text>

<text 
x="55" 
y="298" 
class="s3_266"
>Regularization</text>

<text 
x="191" 
y="298" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0" 
class="s2_266"
>is an approach in which we add to the error term a penalty that gets larger</text>

<text 
x="55" 
y="325" 
class="s2_266"
>as</text>

<text 
x="78" 
y="325" 
class="s4_266"
>beta</text>

<text 
x="126" 
y="325" 
dx="0,0,0,0,0,0,0,0,-0.4,0,0,-1.2,0,0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0" 
class="s2_266"
>gets larger. We then minimize the combined error and penalty. The more</text>

<text 
x="55" 
y="355" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0" 
class="s2_266"
>importance we place on the penalty term, the more we discourage large coefficients.</text>

<text 
x="55" 
y="393" 
class="s2_266"
>For example, in</text>

<text 
x="199" 
y="393" 
dx="0,0,0,0,0,0,0,-0.8,0,0,-0.8,0,0,0,0,0" 
class="s3_266"
>ridge regression</text>

<text 
x="342" 
y="393" 
class="s2_266"
>, we add a penalty proportional to the sum of the squares</text>

<text 
x="55" 
y="421" 
class="s2_266"
>of the</text>

<text 
x="111" 
y="421" 
class="s4_266"
>beta_i</text>

<text 
x="174" 
y="421" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0" 
class="s2_266"
>. (Except that typically we don’t penalize</text>

<text 
x="541" 
y="421" 
class="s4_266"
>beta_0</text>

<text 
x="604" 
y="421" 
class="s2_266"
>, the constant term.)</text>

<text 
x="76" 
y="468" 
dx="0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0" 
class="s5_266"
># alpha is a *hyperparameter* controlling how harsh the penalty is</text>

<text 
x="76" 
y="484" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0" 
class="s5_266"
># sometimes it's called "lambda" but that already means something in Python</text>

<text 
x="76" 
y="499" 
class="s6_266"
>def</text>

<text 
x="108" 
y="499" 
class="s7_266"
>ridge_penalty</text>

<text 
x="211" 
y="499" 
class="s8_266"
>(</text>

<text 
x="219" 
y="499" 
class="s9_266"
>beta</text>

<text 
x="250" 
y="499" 
class="s8_266"
>,</text>

<text 
x="266" 
y="499" 
class="s9_266"
>alpha</text>

<text 
x="306" 
y="499" 
class="s8_266"
>):</text>

<text 
x="92" 
y="514" 
class="s6_266"
>return</text>

<text 
x="148" 
y="514" 
class="s9_266"
>alpha</text>

<text 
x="195" 
y="514" 
class="s8_266"
>*</text>

<text 
x="211" 
y="514" 
class="s9_266"
>dot</text>

<text 
x="235" 
y="514" 
class="s8_266"
>(</text>

<text 
x="242" 
y="514" 
class="s9_266"
>beta</text>

<text 
x="274" 
y="514" 
class="s8_266"
>[</text>

<text 
x="282" 
y="514" 
class="s10_266"
>1</text>

<text 
x="290" 
y="514" 
class="s8_266"
>:],</text>

<text 
x="322" 
y="514" 
class="s9_266"
>beta</text>

<text 
x="353" 
y="514" 
class="s8_266"
>[</text>

<text 
x="361" 
y="514" 
class="s10_266"
>1</text>

<text 
x="369" 
y="514" 
class="s8_266"
>:])</text>

<text 
x="76" 
y="545" 
class="s6_266"
>def</text>

<text 
x="108" 
y="545" 
class="s7_266"
>squared_error_ridge</text>

<text 
x="258" 
y="545" 
class="s8_266"
>(</text>

<text 
x="266" 
y="545" 
class="s9_266"
>x_i</text>

<text 
x="290" 
y="545" 
class="s8_266"
>,</text>

<text 
x="306" 
y="545" 
class="s9_266"
>y_i</text>

<text 
x="329" 
y="545" 
class="s8_266"
>,</text>

<text 
x="345" 
y="545" 
class="s9_266"
>beta</text>

<text 
x="377" 
y="545" 
class="s8_266"
>,</text>

<text 
x="393" 
y="545" 
class="s9_266"
>alpha</text>

<text 
x="432" 
y="545" 
class="s8_266"
>):</text>

<text 
x="108" 
y="561" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0" 
class="s11_266"
>"""estimate error plus ridge penalty on beta"""</text>

<text 
x="108" 
y="576" 
class="s6_266"
>return</text>

<text 
x="163" 
y="576" 
class="s9_266"
>error</text>

<text 
x="203" 
y="576" 
class="s8_266"
>(</text>

<text 
x="211" 
y="576" 
class="s9_266"
>x_i</text>

<text 
x="235" 
y="576" 
class="s8_266"
>,</text>

<text 
x="250" 
y="576" 
class="s9_266"
>y_i</text>

<text 
x="274" 
y="576" 
class="s8_266"
>,</text>

<text 
x="290" 
y="576" 
class="s9_266"
>beta</text>

<text 
x="322" 
y="576" 
dx="0,0,4.6,0" 
class="s8_266"
>) **</text>

<text 
x="361" 
y="576" 
class="s10_266"
>2</text>

<text 
x="377" 
y="576" 
class="s8_266"
>+</text>

<text 
x="393" 
y="576" 
class="s9_266"
>ridge_penalty</text>

<text 
x="495" 
y="576" 
class="s8_266"
>(</text>

<text 
x="503" 
y="576" 
class="s9_266"
>beta</text>

<text 
x="535" 
y="576" 
class="s8_266"
>,</text>

<text 
x="551" 
y="576" 
class="s9_266"
>alpha</text>

<text 
x="590" 
y="576" 
class="s8_266"
>)</text>

<text 
x="55" 
y="629" 
class="s2_266"
>which you can then plug into gradient descent in the usual way:</text>

<text 
x="76" 
y="674" 
class="s6_266"
>def</text>

<text 
x="108" 
y="674" 
class="s7_266"
>ridge_penalty_gradient</text>

<text 
x="282" 
y="674" 
class="s8_266"
>(</text>

<text 
x="290" 
y="674" 
class="s9_266"
>beta</text>

<text 
x="322" 
y="674" 
class="s8_266"
>,</text>

<text 
x="337" 
y="674" 
class="s9_266"
>alpha</text>

<text 
x="377" 
y="674" 
class="s8_266"
>):</text>

<text 
x="108" 
y="689" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0" 
class="s11_266"
>"""gradient of just the ridge penalty"""</text>

<text 
x="108" 
y="705" 
class="s6_266"
>return</text>

<text 
x="163" 
y="705" 
class="s8_266"
>[</text>

<text 
x="171" 
y="705" 
class="s10_266"
>0</text>

<text 
x="179" 
y="705" 
dx="0,0,4.6,0,4.6" 
class="s8_266"
>] + [</text>

<text 
x="219" 
y="705" 
class="s10_266"
>2</text>

<text 
x="235" 
y="705" 
class="s8_266"
>*</text>

<text 
x="250" 
y="705" 
class="s9_266"
>alpha</text>

<text 
x="298" 
y="705" 
class="s8_266"
>*</text>

<text 
x="314" 
y="705" 
class="s9_266"
>beta_j</text>

<text 
x="369" 
y="705" 
class="s6_266"
>for</text>

<text 
x="401" 
y="705" 
class="s9_266"
>beta_j</text>

<text 
x="456" 
y="705" 
class="s12_266"
>in</text>

<text 
x="480" 
y="705" 
class="s9_266"
>beta</text>

<text 
x="511" 
y="705" 
class="s8_266"
>[</text>

<text 
x="519" 
y="705" 
class="s10_266"
>1</text>

<text 
x="527" 
y="705" 
class="s8_266"
>:]]</text>

<text 
x="76" 
y="736" 
class="s6_266"
>def</text>

<text 
x="108" 
y="736" 
class="s7_266"
>squared_error_ridge_gradient</text>

<text 
x="329" 
y="736" 
class="s8_266"
>(</text>

<text 
x="337" 
y="736" 
class="s9_266"
>x_i</text>

<text 
x="361" 
y="736" 
class="s8_266"
>,</text>

<text 
x="377" 
y="736" 
class="s9_266"
>y_i</text>

<text 
x="401" 
y="736" 
class="s8_266"
>,</text>

<text 
x="416" 
y="736" 
class="s9_266"
>beta</text>

<text 
x="448" 
y="736" 
class="s8_266"
>,</text>

<text 
x="464" 
y="736" 
class="s9_266"
>alpha</text>

<text 
x="503" 
y="736" 
class="s8_266"
>):</text>

<text 
x="108" 
y="751" 
dx="0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0" 
class="s11_266"
>"""the gradient corresponding to the ith squared error term</text>

<text 
x="108" 
y="766" 
dx="0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0" 
class="s11_266"
>including the ridge penalty"""</text>

<text 
x="108" 
y="782" 
class="s6_266"
>return</text>

<text 
x="163" 
y="782" 
class="s9_266"
>vector_add</text>

<text 
x="242" 
y="782" 
class="s8_266"
>(</text>

<text 
x="250" 
y="782" 
class="s9_266"
>squared_error_gradient</text>

<text 
x="424" 
y="782" 
class="s8_266"
>(</text>

<text 
x="432" 
y="782" 
class="s9_266"
>x_i</text>

<text 
x="456" 
y="782" 
class="s8_266"
>,</text>

<text 
x="472" 
y="782" 
class="s9_266"
>y_i</text>

<text 
x="495" 
y="782" 
class="s8_266"
>,</text>

<text 
x="511" 
y="782" 
class="s9_266"
>beta</text>

<text 
x="543" 
y="782" 
class="s8_266"
>),</text>

<text 
x="250" 
y="797" 
class="s9_266"
>ridge_penalty_gradient</text>

<text 
x="424" 
y="797" 
class="s8_266"
>(</text>

<text 
x="432" 
y="797" 
class="s9_266"
>beta</text>

<text 
x="464" 
y="797" 
class="s8_266"
>,</text>

<text 
x="480" 
y="797" 
class="s9_266"
>alpha</text>

<text 
x="519" 
y="797" 
class="s8_266"
>))</text>

<text 
x="76" 
y="828" 
class="s6_266"
>def</text>

<text 
x="108" 
y="828" 
class="s7_266"
>estimate_beta_ridge</text>

<text 
x="258" 
y="828" 
class="s8_266"
>(</text>

<text 
x="266" 
y="828" 
class="s9_266"
>x</text>

<text 
x="274" 
y="828" 
class="s8_266"
>,</text>

<text 
x="290" 
y="828" 
class="s9_266"
>y</text>

<text 
x="298" 
y="828" 
class="s8_266"
>,</text>

<text 
x="314" 
y="828" 
class="s9_266"
>alpha</text>

<text 
x="353" 
y="828" 
class="s8_266"
>):</text>

<text 
x="108" 
y="843" 
dx="0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0" 
class="s11_266"
>"""use gradient descent to fit a ridge regression</text>

<text 
x="108" 
y="859" 
dx="0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0" 
class="s11_266"
>with penalty alpha"""</text>

<text 
x="108" 
y="874" 
class="s9_266"
>beta_initial</text>

<text 
x="211" 
y="874" 
dx="0,0,4.6" 
class="s8_266"
>= [</text>

<text 
x="235" 
y="874" 
class="s9_266"
>random</text>

<text 
x="282" 
y="874" 
class="s8_266"
>.</text>

<text 
x="290" 
y="874" 
class="s9_266"
>random</text>

<text 
x="337" 
y="874" 
class="s8_266"
>()</text>

<text 
x="361" 
y="874" 
class="s6_266"
>for</text>

<text 
x="393" 
y="874" 
class="s9_266"
>x_i</text>

<text 
x="424" 
y="874" 
class="s12_266"
>in</text>

<text 
x="448" 
y="874" 
class="s9_266"
>x</text>

<text 
x="456" 
y="874" 
class="s8_266"
>[</text>

<text 
x="464" 
y="874" 
class="s10_266"
>0</text>

<text 
x="472" 
y="874" 
class="s8_266"
>]]</text>

<text 
x="108" 
y="890" 
class="s6_266"
>return</text>

<text 
x="163" 
y="890" 
class="s9_266"
>minimize_stochastic</text>

<text 
x="314" 
y="890" 
class="s8_266"
>(</text>

<text 
x="322" 
y="890" 
class="s9_266"
>partial</text>

<text 
x="377" 
y="890" 
class="s8_266"
>(</text>

<text 
x="385" 
y="890" 
class="s9_266"
>squared_error_ridge</text>

<text 
x="535" 
y="890" 
class="s8_266"
>,</text>

<text 
x="551" 
y="890" 
class="s9_266"
>alpha</text>

<text 
x="590" 
y="890" 
class="s8_266"
>=</text>

<text 
x="598" 
y="890" 
class="s9_266"
>alpha</text>

<text 
x="638" 
y="890" 
class="s8_266"
>),</text>

<text 
x="322" 
y="905" 
class="s9_266"
>partial</text>

<text 
x="377" 
y="905" 
class="s8_266"
>(</text>

<text 
x="385" 
y="905" 
class="s9_266"
>squared_error_ridge_gradient</text>

<text 
x="606" 
y="905" 
class="s8_266"
>,</text>

<text 
x="385" 
y="920" 
class="s9_266"
>alpha</text>

<text 
x="424" 
y="920" 
class="s8_266"
>=</text>

<text 
x="432" 
y="920" 
class="s9_266"
>alpha</text>

<text 
x="472" 
y="920" 
class="s8_266"
>),</text>

<text 
x="322" 
y="936" 
class="s9_266"
>x</text>

<text 
x="329" 
y="936" 
class="s8_266"
>,</text>

<text 
x="345" 
y="936" 
class="s9_266"
>y</text>

<text 
x="353" 
y="936" 
class="s8_266"
>,</text>

<text 
x="322" 
y="951" 
class="s9_266"
>beta_initial</text>

<text 
x="416" 
y="951" 
class="s8_266"
>,</text>

<text 
x="322" 
y="967" 
class="s10_266"
>0.001</text>

<text 
x="361" 
y="967" 
class="s8_266"
>)</text>

<text 
x="55" 
y="1019" 
dx="0,-0.9,0,0" 
class="s2_266"
>With</text>

<text 
x="103" 
y="1019" 
class="s4_266"
>alpha</text>

<text 
x="161" 
y="1019" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_266"
>set to zero, there’s no penalty at all and we get the same results as before:</text>

<text 
x="76" 
y="1067" 
class="s9_266"
>random</text>

<text 
x="124" 
y="1067" 
class="s8_266"
>.</text>

<text 
x="132" 
y="1067" 
class="s9_266"
>seed</text>

<text 
x="163" 
y="1067" 
class="s8_266"
>(</text>

<text 
x="171" 
y="1067" 
class="s10_266"
>0</text>

<text 
x="179" 
y="1067" 
class="s8_266"
>)</text>

<text 
x="76" 
y="1082" 
class="s9_266"
>beta_0</text>

<text 
x="132" 
y="1082" 
class="s8_266"
>=</text>

<text 
x="148" 
y="1082" 
class="s9_266"
>estimate_beta_ridge</text>

<text 
x="298" 
y="1082" 
class="s8_266"
>(</text>

<text 
x="306" 
y="1082" 
class="s9_266"
>x</text>

<text 
x="314" 
y="1082" 
class="s8_266"
>,</text>

<text 
x="329" 
y="1082" 
class="s9_266"
>daily_minutes_good</text>

<text 
x="472" 
y="1082" 
class="s8_266"
>,</text>

<text 
x="487" 
y="1082" 
class="s9_266"
>alpha</text>

<text 
x="527" 
y="1082" 
class="s8_266"
>=</text>

<text 
x="535" 
y="1082" 
class="s10_266"
>0.0</text>

<text 
x="559" 
y="1082" 
class="s8_266"
>)</text>

<text 
x="76" 
y="1098" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s5_266"
># [30.6, 0.97, -1.87, 0.91]</text>

<text 
x="76" 
y="1113" 
class="s9_266"
>dot</text>

<text 
x="100" 
y="1113" 
class="s8_266"
>(</text>

<text 
x="108" 
y="1113" 
class="s9_266"
>beta_0</text>

<text 
x="156" 
y="1113" 
class="s8_266"
>[</text>

<text 
x="163" 
y="1113" 
class="s10_266"
>1</text>

<text 
x="171" 
y="1113" 
class="s8_266"
>:],</text>

<text 
x="203" 
y="1113" 
class="s9_266"
>beta_0</text>

<text 
x="250" 
y="1113" 
class="s8_266"
>[</text>

<text 
x="258" 
y="1113" 
class="s10_266"
>1</text>

<text 
x="266" 
y="1113" 
class="s8_266"
>:])</text>

<text 
x="298" 
y="1113" 
dx="0,0,4.6,0,0,0" 
class="s5_266"
># 5.26</text>

<text 
x="76" 
y="1128" 
class="s9_266"
>multiple_r_squared</text>

<text 
x="219" 
y="1128" 
class="s8_266"
>(</text>

<text 
x="227" 
y="1128" 
class="s9_266"
>x</text>

<text 
x="235" 
y="1128" 
class="s8_266"
>,</text>

<text 
x="250" 
y="1128" 
class="s9_266"
>daily_minutes_good</text>

<text 
x="393" 
y="1128" 
class="s8_266"
>,</text>

<text 
x="408" 
y="1128" 
class="s9_266"
>beta_0</text>

<text 
x="456" 
y="1128" 
class="s8_266"
>)</text>

<text 
x="472" 
y="1128" 
dx="0,0,4.6,0,0,0,0" 
class="s5_266"
># 0.680</text>

<text 
x="55" 
y="1181" 
class="s2_266"
>As we increase</text>

<text 
x="193" 
y="1181" 
class="s4_266"
>alpha</text>

<text 
x="246" 
y="1181" 
class="s2_266"
>, the goodness of fit gets worse, but the size of</text>

<text 
x="659" 
y="1181" 
class="s4_266"
>beta</text>

<text 
x="707" 
y="1181" 
class="s2_266"
>gets smaller:</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
