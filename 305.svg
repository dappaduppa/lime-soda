<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 305 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[

.g1_305{
fill: #BEBEBE;
}

.s1_305{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s2_305{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s3_305{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_305{
font-size: 22.01px;
font-family: LiberationSerif-Bold_b;
fill: #737373;
}
.s5_305{
font-size: 16.50px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s6_305{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s7_305{
font-size: 16.50px;
font-family: LiberationSerif_e;
fill: #8E0012;
}
.s8_305{
font-size: 16.50px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s9_305{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s10_305{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC00FF;
}
.s11_305{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s12_305{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #CC3300;
}
.s13_305{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}
.s14_305{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}
.s15_305{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<path fill-rule="evenodd" d="M93.5,177.1l722.5,0c3,0,5.5,2.5,5.5,5.5l0,99.1c0,3,-2.5,5.5,-5.5,5.5l-722.5,0c-3,0,-5.5,-2.5,-5.5,-5.5l0,-99.1c0,-3,2.5,-5.5,5.5,-5.5m0,1.1l722.5,0c2.4,0,4.4,2,4.4,4.4l0,99.1c0,2.4,-2,4.4,-4.4,4.4l-722.5,0c-2.4,0,-4.4,-2,-4.4,-4.4l0,-99.1c0,-2.4,2,-4.4,4.4,-4.4Z" class="g1_305" />
<text 
x="55" 
y="74" 
class="s1_305"
>Why use</text>

<text 
x="138" 
y="74" 
class="s2_305"
>sigmoid</text>

<text 
x="217" 
y="74" 
class="s1_305"
>instead of the simpler</text>

<text 
x="412" 
y="74" 
class="s2_305"
>step_function</text>

<text 
x="549" 
y="74" 
class="s1_305"
>? In order to train a neural</text>

<text 
x="55" 
y="104" 
class="s1_305"
>network, we’ll need to use calculus, and in order to use calculus, we need</text>

<text 
x="706" 
y="104" 
class="s3_305"
>smooth</text>

<text 
x="55" 
y="132" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s1_305"
>functions. The step function isn’t even continuous, and sigmoid is a good smooth</text>

<text 
x="55" 
y="159" 
class="s1_305"
>approximation of it.</text>

<text 
x="421" 
y="202" 
dx="0,1.1,1.1,1.1" 
class="s4_305"
>NOTE</text>

<text 
x="97" 
y="232" 
dx="0,-1.7,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s5_305"
>You may remember</text>

<text 
x="232" 
y="232" 
class="s6_305"
>sigmoid</text>

<text 
x="291" 
y="232" 
class="s5_305"
>from</text>

<text 
x="327" 
y="232" 
class="s7_305"
>Chapter 16</text>

<text 
x="400" 
y="232" 
class="s5_305"
>, where it was called</text>

<text 
x="539" 
y="232" 
class="s6_305"
>logistic</text>

<text 
x="602" 
y="232" 
dx="0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s5_305"
>. Technically “sigmoid” refers</text>

<text 
x="97" 
y="254" 
class="s5_305"
>to the</text>

<text 
x="139" 
y="254" 
class="s8_305"
>shape</text>

<text 
x="181" 
y="254" 
class="s5_305"
>of the function, “logistic” to this particular function although people often use the terms</text>

<text 
x="97" 
y="273" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.1" 
class="s5_305"
>interchangeably.</text>

<text 
x="55" 
y="319" 
dx="0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s1_305"
>We then calculate the output as:</text>

<text 
x="76" 
y="364" 
class="s9_305"
>def</text>

<text 
x="108" 
y="364" 
class="s10_305"
>neuron_output</text>

<text 
x="211" 
y="364" 
class="s6_305"
>(</text>

<text 
x="219" 
y="364" 
class="s11_305"
>weights</text>

<text 
x="274" 
y="364" 
class="s6_305"
>,</text>

<text 
x="290" 
y="364" 
class="s11_305"
>inputs</text>

<text 
x="337" 
y="364" 
class="s6_305"
>):</text>

<text 
x="108" 
y="379" 
class="s9_305"
>return</text>

<text 
x="163" 
y="379" 
class="s11_305"
>sigmoid</text>

<text 
x="219" 
y="379" 
class="s6_305"
>(</text>

<text 
x="227" 
y="379" 
class="s11_305"
>dot</text>

<text 
x="250" 
y="379" 
class="s6_305"
>(</text>

<text 
x="258" 
y="379" 
class="s11_305"
>weights</text>

<text 
x="314" 
y="379" 
class="s6_305"
>,</text>

<text 
x="329" 
y="379" 
class="s11_305"
>inputs</text>

<text 
x="377" 
y="379" 
class="s6_305"
>))</text>

<text 
x="55" 
y="432" 
class="s1_305"
>Given this function, we can represent a neuron simply as a list of weights whose length is</text>

<text 
x="55" 
y="459" 
class="s1_305"
>one more than the number of inputs to that neuron (because of the bias weight). Then we</text>

<text 
x="55" 
y="487" 
class="s1_305"
>can represent a neural network as a list of (noninput)</text>

<text 
x="523" 
y="487" 
class="s3_305"
>layers</text>

<text 
x="577" 
y="487" 
class="s1_305"
>, where each layer is just a list</text>

<text 
x="55" 
y="514" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2" 
class="s1_305"
>of the neurons in that layer.</text>

<text 
x="55" 
y="553" 
class="s1_305"
>That is, we’ll represent a neural network as a list (layers) of lists (neurons) of lists</text>

<text 
x="55" 
y="580" 
class="s1_305"
>(weights).</text>

<text 
x="55" 
y="619" 
class="s1_305"
>Given such a representation, using the neural network is quite simple:</text>

<text 
x="76" 
y="664" 
class="s9_305"
>def</text>

<text 
x="108" 
y="664" 
class="s10_305"
>feed_forward</text>

<text 
x="203" 
y="664" 
class="s6_305"
>(</text>

<text 
x="211" 
y="664" 
class="s11_305"
>neural_network</text>

<text 
x="322" 
y="664" 
class="s6_305"
>,</text>

<text 
x="337" 
y="664" 
class="s11_305"
>input_vector</text>

<text 
x="432" 
y="664" 
class="s6_305"
>):</text>

<text 
x="108" 
y="679" 
dx="0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0" 
class="s12_305"
>"""takes in a neural network</text>

<text 
x="108" 
y="695" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0,0" 
class="s12_305"
>(represented as a list of lists of lists of weights)</text>

<text 
x="108" 
y="710" 
dx="0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0" 
class="s12_305"
>and returns the output from forward-propagating the input"""</text>

<text 
x="108" 
y="741" 
class="s11_305"
>outputs</text>

<text 
x="171" 
y="741" 
dx="0,0,4.6,0" 
class="s6_305"
>= []</text>

<text 
x="108" 
y="772" 
dx="0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0" 
class="s13_305"
># process one layer at a time</text>

<text 
x="108" 
y="787" 
class="s9_305"
>for</text>

<text 
x="140" 
y="787" 
class="s11_305"
>layer</text>

<text 
x="187" 
y="787" 
class="s14_305"
>in</text>

<text 
x="211" 
y="787" 
class="s11_305"
>neural_network</text>

<text 
x="322" 
y="787" 
class="s6_305"
>:</text>

<text 
x="140" 
y="803" 
class="s11_305"
>input_with_bias</text>

<text 
x="266" 
y="803" 
class="s6_305"
>=</text>

<text 
x="282" 
y="803" 
class="s11_305"
>input_vector</text>

<text 
x="385" 
y="803" 
dx="0,0,4.6" 
class="s6_305"
>+ [</text>

<text 
x="408" 
y="803" 
class="s15_305"
>1</text>

<text 
x="416" 
y="803" 
class="s6_305"
>]</text>

<text 
x="535" 
y="803" 
dx="0,0,4.6,0,0,0,4.6,0,4.6,0,0,0,0,4.6,0,0,0,0" 
class="s13_305"
># add a bias input</text>

<text 
x="140" 
y="818" 
class="s11_305"
>output</text>

<text 
x="195" 
y="818" 
dx="0,0,4.6" 
class="s6_305"
>= [</text>

<text 
x="219" 
y="818" 
class="s11_305"
>neuron_output</text>

<text 
x="322" 
y="818" 
class="s6_305"
>(</text>

<text 
x="329" 
y="818" 
class="s11_305"
>neuron</text>

<text 
x="377" 
y="818" 
class="s6_305"
>,</text>

<text 
x="393" 
y="818" 
class="s11_305"
>input_with_bias</text>

<text 
x="511" 
y="818" 
class="s6_305"
>)</text>

<text 
x="535" 
y="818" 
dx="0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0" 
class="s13_305"
># compute the output</text>

<text 
x="219" 
y="833" 
class="s9_305"
>for</text>

<text 
x="250" 
y="833" 
class="s11_305"
>neuron</text>

<text 
x="306" 
y="833" 
class="s14_305"
>in</text>

<text 
x="329" 
y="833" 
class="s11_305"
>layer</text>

<text 
x="369" 
y="833" 
class="s6_305"
>]</text>

<text 
x="535" 
y="833" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0" 
class="s13_305"
># for each neuron</text>

<text 
x="140" 
y="849" 
class="s11_305"
>outputs</text>

<text 
x="195" 
y="849" 
class="s6_305"
>.</text>

<text 
x="203" 
y="849" 
class="s11_305"
>append</text>

<text 
x="250" 
y="849" 
class="s6_305"
>(</text>

<text 
x="258" 
y="849" 
class="s11_305"
>output</text>

<text 
x="306" 
y="849" 
class="s6_305"
>)</text>

<text 
x="535" 
y="849" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0" 
class="s13_305"
># and remember it</text>

<text 
x="140" 
y="880" 
dx="0,0,4.6,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0" 
class="s13_305"
># then the input to the next layer is the output of this one</text>

<text 
x="140" 
y="895" 
class="s11_305"
>input_vector</text>

<text 
x="242" 
y="895" 
class="s6_305"
>=</text>

<text 
x="258" 
y="895" 
class="s11_305"
>output</text>

<text 
x="108" 
y="926" 
class="s9_305"
>return</text>

<text 
x="163" 
y="926" 
class="s11_305"
>outputs</text>

<text 
x="55" 
y="979" 
dx="0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.8" 
class="s1_305"
>Now it’s easy to build the XOR gate that we couldn’t build with a single perceptron. We</text>

<text 
x="55" 
y="1006" 
class="s1_305"
>just need to scale the weights up so that the</text>

<text 
x="440" 
y="1006" 
class="s2_305"
>neuron_output</text>

<text 
x="578" 
y="1006" 
class="s1_305"
>s are either really close to 0 or</text>

<text 
x="55" 
y="1036" 
class="s1_305"
>really close to 1:</text>

<text 
x="76" 
y="1081" 
class="s11_305"
>xor_network</text>

<text 
x="171" 
y="1081" 
dx="0,0,4.6" 
class="s6_305"
>= [</text>

<text 
x="195" 
y="1081" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s13_305"
># hidden layer</text>

<text 
x="195" 
y="1096" 
class="s6_305"
>[[</text>

<text 
x="211" 
y="1096" 
class="s15_305"
>20</text>

<text 
x="227" 
y="1096" 
class="s6_305"
>,</text>

<text 
x="242" 
y="1096" 
class="s15_305"
>20</text>

<text 
x="258" 
y="1096" 
dx="0,0,4.6" 
class="s6_305"
>, -</text>

<text 
x="282" 
y="1096" 
class="s15_305"
>30</text>

<text 
x="298" 
y="1096" 
class="s6_305"
>],</text>

<text 
x="361" 
y="1096" 
dx="0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0" 
class="s13_305"
># 'and' neuron</text>

<text 
x="203" 
y="1112" 
class="s6_305"
>[</text>

<text 
x="211" 
y="1112" 
class="s15_305"
>20</text>

<text 
x="227" 
y="1112" 
class="s6_305"
>,</text>

<text 
x="242" 
y="1112" 
class="s15_305"
>20</text>

<text 
x="258" 
y="1112" 
dx="0,0,4.6" 
class="s6_305"
>, -</text>

<text 
x="282" 
y="1112" 
class="s15_305"
>10</text>

<text 
x="298" 
y="1112" 
class="s6_305"
>]],</text>

<text 
x="361" 
y="1112" 
dx="0,0,4.6,0,0,0" 
class="s13_305"
># 'or'</text>

<text 
x="424" 
y="1112" 
class="s13_305"
>neuron</text>

<text 
x="195" 
y="1127" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s13_305"
># output layer</text>

<text 
x="195" 
y="1143" 
class="s6_305"
>[[-</text>

<text 
x="219" 
y="1143" 
class="s15_305"
>60</text>

<text 
x="235" 
y="1143" 
class="s6_305"
>,</text>

<text 
x="250" 
y="1143" 
class="s15_305"
>60</text>

<text 
x="266" 
y="1143" 
dx="0,0,4.6" 
class="s6_305"
>, -</text>

<text 
x="290" 
y="1143" 
class="s15_305"
>30</text>

<text 
x="306" 
y="1143" 
class="s6_305"
>]]]</text>

<text 
x="361" 
y="1143" 
dx="0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0" 
class="s13_305"
># '2nd input but not 1st input' neuron</text>

<text 
x="76" 
y="1173" 
class="s9_305"
>for</text>

<text 
x="108" 
y="1173" 
class="s11_305"
>x</text>

<text 
x="124" 
y="1173" 
class="s14_305"
>in</text>

<text 
x="148" 
y="1173" 
class="s6_305"
>[</text>

<text 
x="156" 
y="1173" 
class="s15_305"
>0</text>

<text 
x="163" 
y="1173" 
class="s6_305"
>,</text>

<text 
x="179" 
y="1173" 
class="s15_305"
>1</text>

<text 
x="187" 
y="1173" 
class="s6_305"
>]:</text>

<text 
x="108" 
y="1189" 
class="s9_305"
>for</text>

<text 
x="140" 
y="1189" 
class="s11_305"
>y</text>

<text 
x="156" 
y="1189" 
class="s14_305"
>in</text>

<text 
x="179" 
y="1189" 
class="s6_305"
>[</text>

<text 
x="187" 
y="1189" 
class="s15_305"
>0</text>

<text 
x="195" 
y="1189" 
class="s6_305"
>,</text>

<text 
x="211" 
y="1189" 
class="s15_305"
>1</text>

<text 
x="219" 
y="1189" 
class="s6_305"
>]:</text>

<text 
x="140" 
y="1204" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0" 
class="s13_305"
># feed_forward produces the outputs of every neuron</text>

<text 
x="140" 
y="1220" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0" 
class="s13_305"
># feed_forward[-1] is the outputs of the output-layer neurons</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

]]></style>

</svg>
