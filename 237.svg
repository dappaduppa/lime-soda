<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 237 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_237{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_237{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_237{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
dx="0,0,-1.7,0,0,-0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s1_237"
>A More Sophisticated Spam Filter</text>

<text 
x="55" 
y="122" 
class="s2_237"
>Imagine now that we have a vocabulary of many words</text>

<image preserveAspectRatio="none" x="549" y="102" width="116" height="20" xlink:href="237/img/1.png" />
<text 
x="664" 
y="122" 
dx="0,0,0,-1.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>. To move this into</text>

<text 
x="55" 
y="154" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>the realm of probability theory, we’ll write</text>

<image preserveAspectRatio="none" x="435" y="129" width="26" height="25" xlink:href="237/img/2.png" />
<text 
x="466" 
y="154" 
class="s2_237"
>for the event “a message contains the word</text>

<image preserveAspectRatio="none" x="55" y="162" width="24" height="20" xlink:href="237/img/3.png" />
<text 
x="79" 
y="181" 
class="s2_237"
>.” Also imagine that (through some unspecified-at-this-point process) we’ve come up</text>

<text 
x="55" 
y="214" 
class="s2_237"
>with an estimate</text>

<image preserveAspectRatio="none" x="205" y="188" width="93" height="27" xlink:href="237/img/4.png" />
<text 
x="303" 
y="214" 
class="s2_237"
>for the probability that a spam message contains the</text>

<text 
x="765" 
y="214" 
class="s3_237"
>i</text>

<text 
x="771" 
y="214" 
class="s2_237"
>th word,</text>

<text 
x="55" 
y="247" 
class="s2_237"
>and a similar estimate</text>

<image preserveAspectRatio="none" x="253" y="221" width="123" height="27" xlink:href="237/img/5.png" />
<text 
x="381" 
y="247" 
class="s2_237"
>for the probability that a nonspam message contains</text>

<text 
x="55" 
y="275" 
class="s2_237"
>the</text>

<text 
x="87" 
y="275" 
class="s3_237"
>i</text>

<text 
x="93" 
y="275" 
class="s2_237"
>th word.</text>

<text 
x="55" 
y="313" 
class="s2_237"
>The key to Naive Bayes is making the (big) assumption that the presences (or absences) of</text>

<text 
x="55" 
y="341" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>each word are independent of one another, conditional on a message being spam or not.</text>

<text 
x="55" 
y="368" 
dx="0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>Intuitively, this assumption means that knowing whether a certain spam message contains</text>

<text 
x="55" 
y="396" 
class="s2_237"
>the word “viagra” gives you no information about whether that same message contains the</text>

<text 
x="55" 
y="423" 
class="s2_237"
>word “rolex.” In math terms, this means that:</text>

<image preserveAspectRatio="none" x="55" y="441" width="799" height="30" xlink:href="237/img/6.png" />
<text 
x="55" 
y="501" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>This is an extreme assumption. (There’s a reason the technique has “naive” in its name.)</text>

<text 
x="55" 
y="529" 
class="s2_237"
>Imagine that our vocabulary consists</text>

<text 
x="384" 
y="529" 
class="s3_237"
>only</text>

<text 
x="427" 
y="529" 
class="s2_237"
>of the words “viagra” and “rolex,” and that half</text>

<text 
x="55" 
y="556" 
class="s2_237"
>of all spam messages are for “cheap viagra” and that the other half are for “authentic</text>

<text 
x="55" 
y="584" 
class="s2_237"
>rolex.” In this case, the Naive Bayes estimate that a spam message contains both “viagra”</text>

<text 
x="55" 
y="611" 
class="s2_237"
>and “rolex” is:</text>

<image preserveAspectRatio="none" x="55" y="629" width="799" height="30" xlink:href="237/img/7.png" />
<text 
x="55" 
y="690" 
class="s2_237"
>since we’ve assumed away the knowledge that “viagra” and “rolex” actually never occur</text>

<text 
x="55" 
y="718" 
dx="0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>together. Despite the unrealisticness of this assumption, this model often performs well</text>

<text 
x="55" 
y="745" 
class="s2_237"
>and is used in actual spam filters.</text>

<text 
x="55" 
y="784" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>The same Bayes’s Theorem reasoning we used for our “viagra-only” spam filter tells us</text>

<text 
x="55" 
y="811" 
class="s2_237"
>that we can calculate the probability a message is spam using the equation:</text>

<image preserveAspectRatio="none" x="55" y="830" width="799" height="30" xlink:href="237/img/8.png" />
<text 
x="55" 
y="892" 
class="s2_237"
>The Naive Bayes assumption allows us to compute each of the probabilities on the right</text>

<text 
x="55" 
y="919" 
class="s2_237"
>simply by multiplying together the individual probability estimates for each vocabulary</text>

<text 
x="55" 
y="947" 
class="s2_237"
>word.</text>

<text 
x="55" 
y="985" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a</text>

<text 
x="55" 
y="1013" 
class="s2_237"
>problem called</text>

<text 
x="191" 
y="1013" 
class="s3_237"
>underflow</text>

<text 
x="280" 
y="1013" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_237"
>, in which computers don’t deal well with floating-point numbers</text>

<text 
x="55" 
y="1040" 
class="s2_237"
>that are too close to zero. Recalling from algebra that</text>

<image preserveAspectRatio="none" x="55" y="1047" width="331" height="31" xlink:href="237/img/9.png" />
<text 
x="391" 
y="1078" 
class="s2_237"
>and that</text>

<image preserveAspectRatio="none" x="467" y="1047" width="212" height="31" xlink:href="237/img/10.png" />
<text 
x="678" 
y="1078" 
class="s2_237"
>, we usually</text>

<text 
x="55" 
y="1111" 
class="s2_237"
>compute</text>

<image preserveAspectRatio="none" x="136" y="1085" width="126" height="26" xlink:href="237/img/11.png" />
<text 
x="267" 
y="1111" 
class="s2_237"
>as the equivalent (but floating-point-friendlier):</text>

<image preserveAspectRatio="none" x="55" y="1129" width="799" height="67" xlink:href="237/img/12.png" />


<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
