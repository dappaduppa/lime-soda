<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 277 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_277{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_277{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_277{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s4_277{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s5_277{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}
.s6_277{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}
.s7_277{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s8_277{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}
.s9_277{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #336666;
}
.s10_277{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
class="s1_277"
>Applying the Model</text>

<text 
x="55" 
y="122" 
dx="0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>We’ll want to split our data into a training set and a test set:</text>

<text 
x="76" 
y="167" 
class="s3_277"
>random</text>

<text 
x="124" 
y="167" 
class="s4_277"
>.</text>

<text 
x="132" 
y="167" 
class="s3_277"
>seed</text>

<text 
x="163" 
y="167" 
class="s4_277"
>(</text>

<text 
x="171" 
y="167" 
class="s5_277"
>0</text>

<text 
x="179" 
y="167" 
class="s4_277"
>)</text>

<text 
x="76" 
y="182" 
class="s3_277"
>x_train</text>

<text 
x="132" 
y="182" 
class="s4_277"
>,</text>

<text 
x="148" 
y="182" 
class="s3_277"
>x_test</text>

<text 
x="195" 
y="182" 
class="s4_277"
>,</text>

<text 
x="211" 
y="182" 
class="s3_277"
>y_train</text>

<text 
x="266" 
y="182" 
class="s4_277"
>,</text>

<text 
x="282" 
y="182" 
class="s3_277"
>y_test</text>

<text 
x="337" 
y="182" 
class="s4_277"
>=</text>

<text 
x="353" 
y="182" 
class="s3_277"
>train_test_split</text>

<text 
x="480" 
y="182" 
class="s4_277"
>(</text>

<text 
x="487" 
y="182" 
class="s3_277"
>rescaled_x</text>

<text 
x="567" 
y="182" 
class="s4_277"
>,</text>

<text 
x="582" 
y="182" 
class="s3_277"
>y</text>

<text 
x="590" 
y="182" 
class="s4_277"
>,</text>

<text 
x="606" 
y="182" 
class="s5_277"
>0.33</text>

<text 
x="638" 
y="182" 
class="s4_277"
>)</text>

<text 
x="76" 
y="213" 
dx="0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0" 
class="s6_277"
># want to maximize log likelihood on the training data</text>

<text 
x="76" 
y="228" 
class="s3_277"
>fn</text>

<text 
x="100" 
y="228" 
class="s4_277"
>=</text>

<text 
x="116" 
y="228" 
class="s3_277"
>partial</text>

<text 
x="171" 
y="228" 
class="s4_277"
>(</text>

<text 
x="179" 
y="228" 
class="s3_277"
>logistic_log_likelihood</text>

<text 
x="361" 
y="228" 
class="s4_277"
>,</text>

<text 
x="377" 
y="228" 
class="s3_277"
>x_train</text>

<text 
x="432" 
y="228" 
class="s4_277"
>,</text>

<text 
x="448" 
y="228" 
class="s3_277"
>y_train</text>

<text 
x="503" 
y="228" 
class="s4_277"
>)</text>

<text 
x="76" 
y="244" 
class="s3_277"
>gradient_fn</text>

<text 
x="171" 
y="244" 
class="s4_277"
>=</text>

<text 
x="187" 
y="244" 
class="s3_277"
>partial</text>

<text 
x="242" 
y="244" 
class="s4_277"
>(</text>

<text 
x="250" 
y="244" 
class="s3_277"
>logistic_log_gradient</text>

<text 
x="416" 
y="244" 
class="s4_277"
>,</text>

<text 
x="432" 
y="244" 
class="s3_277"
>x_train</text>

<text 
x="487" 
y="244" 
class="s4_277"
>,</text>

<text 
x="503" 
y="244" 
class="s3_277"
>y_train</text>

<text 
x="559" 
y="244" 
class="s4_277"
>)</text>

<text 
x="76" 
y="275" 
dx="0,0,4.6,0,0,0,0,4.6,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s6_277"
># pick a random starting point</text>

<text 
x="76" 
y="290" 
class="s3_277"
>beta_0</text>

<text 
x="132" 
y="290" 
dx="0,0,4.6" 
class="s4_277"
>= [</text>

<text 
x="156" 
y="290" 
class="s3_277"
>random</text>

<text 
x="203" 
y="290" 
class="s4_277"
>.</text>

<text 
x="211" 
y="290" 
class="s3_277"
>random</text>

<text 
x="258" 
y="290" 
class="s4_277"
>()</text>

<text 
x="282" 
y="290" 
class="s7_277"
>for</text>

<text 
x="314" 
y="290" 
class="s3_277"
>_</text>

<text 
x="329" 
y="290" 
class="s8_277"
>in</text>

<text 
x="353" 
y="290" 
class="s9_277"
>range</text>

<text 
x="393" 
y="290" 
class="s4_277"
>(</text>

<text 
x="401" 
y="290" 
class="s5_277"
>3</text>

<text 
x="408" 
y="290" 
class="s4_277"
>)]</text>

<text 
x="76" 
y="321" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0" 
class="s6_277"
># and maximize using gradient descent</text>

<text 
x="76" 
y="336" 
class="s3_277"
>beta_hat</text>

<text 
x="148" 
y="336" 
class="s4_277"
>=</text>

<text 
x="163" 
y="336" 
class="s3_277"
>maximize_batch</text>

<text 
x="274" 
y="336" 
class="s4_277"
>(</text>

<text 
x="282" 
y="336" 
class="s3_277"
>fn</text>

<text 
x="298" 
y="336" 
class="s4_277"
>,</text>

<text 
x="314" 
y="336" 
class="s3_277"
>gradient_fn</text>

<text 
x="401" 
y="336" 
class="s4_277"
>,</text>

<text 
x="416" 
y="336" 
class="s3_277"
>beta_0</text>

<text 
x="464" 
y="336" 
class="s4_277"
>)</text>

<text 
x="55" 
y="389" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>Alternatively, you could use stochastic gradient descent:</text>

<text 
x="76" 
y="434" 
class="s3_277"
>beta_hat</text>

<text 
x="148" 
y="434" 
class="s4_277"
>=</text>

<text 
x="163" 
y="434" 
class="s3_277"
>maximize_stochastic</text>

<text 
x="314" 
y="434" 
class="s4_277"
>(</text>

<text 
x="322" 
y="434" 
class="s3_277"
>logistic_log_likelihood_i</text>

<text 
x="519" 
y="434" 
class="s4_277"
>,</text>

<text 
x="322" 
y="450" 
class="s3_277"
>logistic_log_gradient_i</text>

<text 
x="503" 
y="450" 
class="s4_277"
>,</text>

<text 
x="322" 
y="465" 
class="s3_277"
>x_train</text>

<text 
x="377" 
y="465" 
class="s4_277"
>,</text>

<text 
x="393" 
y="465" 
class="s3_277"
>y_train</text>

<text 
x="448" 
y="465" 
class="s4_277"
>,</text>

<text 
x="464" 
y="465" 
class="s3_277"
>beta_0</text>

<text 
x="511" 
y="465" 
class="s4_277"
>)</text>

<text 
x="55" 
y="518" 
class="s2_277"
>Either way we find approximately:</text>

<text 
x="76" 
y="563" 
class="s3_277"
>beta_hat</text>

<text 
x="148" 
y="563" 
dx="0,0,4.6,0" 
class="s4_277"
>= [-</text>

<text 
x="179" 
y="563" 
class="s5_277"
>1.90</text>

<text 
x="211" 
y="563" 
class="s4_277"
>,</text>

<text 
x="227" 
y="563" 
class="s5_277"
>4.05</text>

<text 
x="258" 
y="563" 
dx="0,0,4.6" 
class="s4_277"
>, -</text>

<text 
x="282" 
y="563" 
class="s5_277"
>3.87</text>

<text 
x="314" 
y="563" 
class="s4_277"
>]</text>

<text 
x="55" 
y="616" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>These are coefficients for the</text>

<text 
x="316" 
y="616" 
class="s10_277"
>rescale</text>

<text 
x="390" 
y="616" 
class="s2_277"
>d data, but we can transform them back to the</text>

<text 
x="55" 
y="645" 
class="s2_277"
>original data as well:</text>

<text 
x="76" 
y="690" 
class="s3_277"
>beta_hat_unscaled</text>

<text 
x="219" 
y="690" 
dx="0,0,4.6" 
class="s4_277"
>= [</text>

<text 
x="242" 
y="690" 
class="s5_277"
>7.61</text>

<text 
x="274" 
y="690" 
class="s4_277"
>,</text>

<text 
x="290" 
y="690" 
class="s5_277"
>1.42</text>

<text 
x="322" 
y="690" 
dx="0,0,4.6" 
class="s4_277"
>, -</text>

<text 
x="345" 
y="690" 
class="s5_277"
>0.000249</text>

<text 
x="408" 
y="690" 
class="s4_277"
>]</text>

<text 
x="55" 
y="743" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>Unfortunately, these are not as easy to interpret as linear regression coefficients. All else</text>

<text 
x="55" 
y="771" 
class="s2_277"
>being equal, an extra year of experience adds 1.42 to the input of</text>

<text 
x="630" 
y="771" 
class="s10_277"
>logistic</text>

<text 
x="715" 
y="771" 
class="s2_277"
>. All else being</text>

<text 
x="55" 
y="800" 
class="s2_277"
>equal, an extra $10,000 of salary subtracts 2.49 from the input of</text>

<text 
x="630" 
y="800" 
class="s10_277"
>logistic</text>

<text 
x="715" 
y="800" 
class="s2_277"
>.</text>

<text 
x="55" 
y="841" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>The impact on the output, however, depends on the other inputs as well. If</text>

<text 
x="712" 
y="841" 
class="s10_277"
>dot(beta,</text>

<text 
x="55" 
y="871" 
class="s10_277"
>x_i)</text>

<text 
x="102" 
y="871" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>is already large (corresponding to a probability close to 1), increasing it even by a lot</text>

<text 
x="55" 
y="901" 
dx="0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_277"
>cannot affect the probability very much. If it’s close to 0, increasing it just a little might</text>

<text 
x="55" 
y="928" 
class="s2_277"
>increase the probability quite a bit.</text>

<text 
x="55" 
y="967" 
class="s2_277"
>What we can say is that — all else being equal — people with more experience are more</text>

<text 
x="55" 
y="994" 
class="s2_277"
>likely to pay for accounts. And that — all else being equal — people with higher salaries</text>

<text 
x="55" 
y="1022" 
class="s2_277"
>are less likely to pay for accounts. (This was also somewhat apparent when we plotted the</text>

<text 
x="55" 
y="1049" 
class="s2_277"
>data.)</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
