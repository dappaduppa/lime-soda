<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 289 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_289{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_289{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_289{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s4_289{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s5_289{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #8E0012;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
dx="0,0,0,0,-0.6,0,0" 
class="s1_289"
>Entropy</text>

<text 
x="55" 
y="122" 
class="s2_289"
>In order to build a decision tree, we will need to decide what questions to ask and in what</text>

<text 
x="55" 
y="149" 
dx="0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>order. At each stage of the tree there are some possibilities we’ve eliminated and some that</text>

<text 
x="55" 
y="177" 
dx="0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>we haven’t. After learning that an animal doesn’t have more than five legs, we’ve</text>

<text 
x="55" 
y="204" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,-1.8,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>eliminated the possibility that it’s a grasshopper. We haven’t eliminated the possibility that</text>

<text 
x="55" 
y="232" 
dx="0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>it’s a duck. Every possible question partitions the remaining possibilities according to their</text>

<text 
x="55" 
y="259" 
class="s2_289"
>answers.</text>

<text 
x="55" 
y="298" 
dx="0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>Ideally, we’d like to choose questions whose answers give a lot of information about what</text>

<text 
x="55" 
y="325" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>our tree should predict. If there’s a single yes/no question for which “yes” answers always</text>

<text 
x="55" 
y="353" 
class="s2_289"
>correspond to</text>

<text 
x="180" 
y="353" 
class="s3_289"
>True</text>

<text 
x="228" 
y="353" 
class="s2_289"
>outputs and “no” answers to</text>

<text 
x="481" 
y="353" 
class="s3_289"
>False</text>

<text 
x="540" 
y="353" 
class="s2_289"
>outputs (or vice versa), this would</text>

<text 
x="55" 
y="382" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>be an awesome question to pick. Conversely, a yes/no question for which neither answer</text>

<text 
x="55" 
y="410" 
class="s2_289"
>gives you much new information about what the prediction should be is probably not a</text>

<text 
x="55" 
y="437" 
class="s2_289"
>good choice.</text>

<text 
x="55" 
y="476" 
dx="0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>We capture this notion of “how much information” with</text>

<text 
x="553" 
y="476" 
dx="0,0,0,0,-0.8,0,0" 
class="s4_289"
>entropy</text>

<text 
x="619" 
y="476" 
dx="0,0,0,-2.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>. You have probably heard</text>

<text 
x="55" 
y="503" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>this used to mean disorder. We use it to represent the uncertainty associated with data.</text>

<text 
x="55" 
y="542" 
class="s2_289"
>Imagine that we have a set</text>

<text 
x="294" 
y="542" 
class="s4_289"
>S</text>

<text 
x="310" 
y="542" 
class="s2_289"
>of data, each member of which is labeled as belonging to one</text>

<text 
x="55" 
y="574" 
class="s2_289"
>of a finite number of classes</text>

<image preserveAspectRatio="none" x="309" y="549" width="113" height="25" xlink:href="289/img/1.png" />
<text 
x="422" 
y="574" 
class="s2_289"
>. If all the data points belong to a single class,</text>

<text 
x="55" 
y="601" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>then there is no real uncertainty, which means we’d like there to be low entropy. If the data</text>

<text 
x="55" 
y="629" 
class="s2_289"
>points are evenly spread across the classes, there is a lot of uncertainty and we’d like there</text>

<text 
x="55" 
y="656" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4" 
class="s2_289"
>to be high entropy.</text>

<text 
x="55" 
y="695" 
class="s2_289"
>In math terms, if</text>

<image preserveAspectRatio="none" x="207" y="674" width="18" height="21" xlink:href="289/img/2.png" />
<text 
x="231" 
y="695" 
class="s2_289"
>is the proportion of data labeled as class</text>

<image preserveAspectRatio="none" x="589" y="676" width="18" height="19" xlink:href="289/img/3.png" />
<text 
x="606" 
y="695" 
class="s2_289"
>, we define the entropy as:</text>

<image preserveAspectRatio="none" x="55" y="713" width="799" height="58" xlink:href="289/img/4.png" />
<text 
x="55" 
y="813" 
class="s2_289"
>with the (standard) convention that</text>

<image preserveAspectRatio="none" x="368" y="782" width="150" height="31" xlink:href="289/img/5.png" />
<text 
x="517" 
y="813" 
class="s2_289"
>.</text>

<text 
x="55" 
y="858" 
dx="0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_289"
>Without worrying too much about the grisly details, each term</text>

<image preserveAspectRatio="none" x="608" y="831" width="97" height="27" xlink:href="289/img/6.png" />
<text 
x="710" 
y="858" 
class="s2_289"
>is non-negative</text>

<text 
x="55" 
y="885" 
class="s2_289"
>and is close to zero precisely when</text>

<image preserveAspectRatio="none" x="367" y="865" width="19" height="21" xlink:href="289/img/7.png" />
<text 
x="391" 
y="885" 
class="s2_289"
>is either close to zero or close to one (</text>

<text 
x="725" 
y="885" 
class="s5_289"
>Figure 17-2</text>

<text 
x="828" 
y="885" 
class="s2_289"
>).</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
