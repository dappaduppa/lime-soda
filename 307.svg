<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 307 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_307{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_307{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_307{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_307{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s5_307{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s6_307{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC00FF;
}
.s7_307{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s8_307{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s9_307{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}
.s10_307{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}
.s11_307{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}
.s12_307{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #336666;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
dx="0,0,0,0,0,0,-0.6,0,0,0,0,0,0,0,0" 
class="s1_307"
>Backpropagation</text>

<text 
x="55" 
y="122" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>Usually we don’t build neural networks by hand. This is in part because we use them to</text>

<text 
x="55" 
y="149" 
class="s2_307"
>solve much bigger problems — an image recognition problem might involve hundreds or</text>

<text 
x="55" 
y="177" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>thousands of neurons. And it’s in part because we usually won’t be able to “reason out”</text>

<text 
x="55" 
y="204" 
class="s2_307"
>what the neurons should be.</text>

<text 
x="55" 
y="243" 
class="s2_307"
>Instead (as usual) we use data to</text>

<text 
x="344" 
y="243" 
class="s3_307"
>train</text>

<text 
x="393" 
y="243" 
class="s2_307"
>neural networks. One popular approach is an</text>

<text 
x="55" 
y="270" 
class="s2_307"
>algorithm called</text>

<text 
x="203" 
y="270" 
dx="0,0,0,0,0,0,-0.8,0,0,0,0,0,0,0,0" 
class="s3_307"
>backpropagation</text>

<text 
x="358" 
y="270" 
class="s2_307"
>that has similarities to the gradient descent algorithm</text>

<text 
x="55" 
y="298" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2" 
class="s2_307"
>we looked at earlier.</text>

<text 
x="55" 
y="336" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0" 
class="s2_307"
>Imagine we have a training set that consists of input vectors and corresponding target</text>

<text 
x="55" 
y="364" 
class="s2_307"
>output vectors. For example, in our previous</text>

<text 
x="451" 
y="364" 
class="s4_307"
>xor_network</text>

<text 
x="572" 
y="364" 
class="s2_307"
>example, the input vector</text>

<text 
x="801" 
y="364" 
class="s4_307"
>[1,</text>

<text 
x="55" 
y="393" 
class="s4_307"
>0]</text>

<text 
x="81" 
y="393" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>corresponded to the target output</text>

<text 
x="377" 
y="393" 
class="s4_307"
>[1]</text>

<text 
x="408" 
y="393" 
class="s2_307"
>. And imagine that our network has some set of</text>

<text 
x="55" 
y="423" 
dx="0,0,0,0,0,0,0,0,0,0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>weights. We then adjust the weights using the following algorithm:</text>

<text 
x="75" 
y="461" 
dx="0,0,0,6.6,0,0" 
class="s2_307"
>1. Run</text>

<text 
x="146" 
y="461" 
class="s4_307"
>feed_forward</text>

<text 
x="278" 
y="461" 
class="s2_307"
>on an input vector to produce the outputs of all the neurons in the</text>

<text 
x="104" 
y="486" 
class="s2_307"
>network.</text>

<text 
x="75" 
y="533" 
dx="0,0,0,6.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>2. This results in an error for each output neuron — the difference between its output</text>

<text 
x="104" 
y="558" 
dx="0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0" 
class="s2_307"
>and its target.</text>

<text 
x="75" 
y="606" 
dx="0,0,0,6.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>3. Compute the gradient of this error as a function of the neuron’s weights, and adjust</text>

<text 
x="104" 
y="631" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2" 
class="s2_307"
>its weights in the direction that most decreases the error.</text>

<text 
x="75" 
y="678" 
dx="0,0,0,6.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2" 
class="s2_307"
>4. “Propagate” these output errors backward to infer errors for the hidden layer.</text>

<text 
x="75" 
y="726" 
dx="0,0,0,6.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>5. Compute the gradients of these errors and adjust the hidden layer’s weights in the</text>

<text 
x="104" 
y="751" 
dx="0,0,0,0,0,0,0,0,0,0,0,-1.2" 
class="s2_307"
>same manner.</text>

<text 
x="55" 
y="799" 
dx="0,-1.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_307"
>Typically we run this algorithm many times for our entire training set until the network</text>

<text 
x="55" 
y="827" 
dx="0,0,0,0,0,0,-0.4,0,0,0" 
class="s2_307"
>converges:</text>

<text 
x="76" 
y="872" 
class="s5_307"
>def</text>

<text 
x="108" 
y="872" 
class="s6_307"
>backpropagate</text>

<text 
x="211" 
y="872" 
class="s7_307"
>(</text>

<text 
x="219" 
y="872" 
class="s8_307"
>network</text>

<text 
x="274" 
y="872" 
class="s7_307"
>,</text>

<text 
x="290" 
y="872" 
class="s8_307"
>input_vector</text>

<text 
x="385" 
y="872" 
class="s7_307"
>,</text>

<text 
x="401" 
y="872" 
class="s8_307"
>targets</text>

<text 
x="456" 
y="872" 
class="s7_307"
>):</text>

<text 
x="108" 
y="903" 
class="s8_307"
>hidden_outputs</text>

<text 
x="219" 
y="903" 
class="s7_307"
>,</text>

<text 
x="235" 
y="903" 
class="s8_307"
>outputs</text>

<text 
x="298" 
y="903" 
class="s7_307"
>=</text>

<text 
x="314" 
y="903" 
class="s8_307"
>feed_forward</text>

<text 
x="408" 
y="903" 
class="s7_307"
>(</text>

<text 
x="416" 
y="903" 
class="s8_307"
>network</text>

<text 
x="472" 
y="903" 
class="s7_307"
>,</text>

<text 
x="487" 
y="903" 
class="s8_307"
>input_vector</text>

<text 
x="582" 
y="903" 
class="s7_307"
>)</text>

<text 
x="108" 
y="934" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,4.6,0,0,4.6,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0" 
class="s9_307"
># the output * (1 - output) is from the derivative of sigmoid</text>

<text 
x="108" 
y="949" 
class="s8_307"
>output_deltas</text>

<text 
x="219" 
y="949" 
dx="0,0,4.6" 
class="s7_307"
>= [</text>

<text 
x="242" 
y="949" 
class="s8_307"
>output</text>

<text 
x="298" 
y="949" 
dx="0,0,4.6" 
class="s7_307"
>* (</text>

<text 
x="322" 
y="949" 
class="s10_307"
>1</text>

<text 
x="337" 
y="949" 
class="s7_307"
>-</text>

<text 
x="353" 
y="949" 
class="s8_307"
>output</text>

<text 
x="401" 
y="949" 
dx="0,0,4.6,0,4.6" 
class="s7_307"
>) * (</text>

<text 
x="440" 
y="949" 
class="s8_307"
>output</text>

<text 
x="495" 
y="949" 
class="s7_307"
>-</text>

<text 
x="511" 
y="949" 
class="s8_307"
>target</text>

<text 
x="559" 
y="949" 
class="s7_307"
>)</text>

<text 
x="242" 
y="964" 
class="s5_307"
>for</text>

<text 
x="274" 
y="964" 
class="s8_307"
>output</text>

<text 
x="322" 
y="964" 
class="s7_307"
>,</text>

<text 
x="337" 
y="964" 
class="s8_307"
>target</text>

<text 
x="393" 
y="964" 
class="s11_307"
>in</text>

<text 
x="416" 
y="964" 
class="s12_307"
>zip</text>

<text 
x="440" 
y="964" 
class="s7_307"
>(</text>

<text 
x="448" 
y="964" 
class="s8_307"
>outputs</text>

<text 
x="503" 
y="964" 
class="s7_307"
>,</text>

<text 
x="519" 
y="964" 
class="s8_307"
>targets</text>

<text 
x="574" 
y="964" 
class="s7_307"
>)]</text>

<text 
x="108" 
y="995" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0" 
class="s9_307"
># adjust weights for output layer, one neuron at a time</text>

<text 
x="108" 
y="1011" 
class="s5_307"
>for</text>

<text 
x="140" 
y="1011" 
class="s8_307"
>i</text>

<text 
x="148" 
y="1011" 
class="s7_307"
>,</text>

<text 
x="163" 
y="1011" 
class="s8_307"
>output_neuron</text>

<text 
x="274" 
y="1011" 
class="s11_307"
>in</text>

<text 
x="298" 
y="1011" 
class="s12_307"
>enumerate</text>

<text 
x="369" 
y="1011" 
class="s7_307"
>(</text>

<text 
x="377" 
y="1011" 
class="s8_307"
>network</text>

<text 
x="432" 
y="1011" 
class="s7_307"
>[-</text>

<text 
x="448" 
y="1011" 
class="s10_307"
>1</text>

<text 
x="456" 
y="1011" 
class="s7_307"
>]):</text>

<text 
x="140" 
y="1026" 
dx="0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0" 
class="s9_307"
># focus on the ith output layer neuron</text>

<text 
x="140" 
y="1041" 
class="s5_307"
>for</text>

<text 
x="171" 
y="1041" 
class="s8_307"
>j</text>

<text 
x="179" 
y="1041" 
class="s7_307"
>,</text>

<text 
x="195" 
y="1041" 
class="s8_307"
>hidden_output</text>

<text 
x="306" 
y="1041" 
class="s11_307"
>in</text>

<text 
x="329" 
y="1041" 
class="s12_307"
>enumerate</text>

<text 
x="401" 
y="1041" 
class="s7_307"
>(</text>

<text 
x="408" 
y="1041" 
class="s8_307"
>hidden_outputs</text>

<text 
x="527" 
y="1041" 
dx="0,0,4.6" 
class="s7_307"
>+ [</text>

<text 
x="551" 
y="1041" 
class="s10_307"
>1</text>

<text 
x="559" 
y="1041" 
class="s7_307"
>]):</text>

<text 
x="171" 
y="1057" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,4.6,0,0,0" 
class="s9_307"
># adjust the jth weight based on both</text>

<text 
x="171" 
y="1072" 
dx="0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,4.6,0,0,0,0" 
class="s9_307"
># this neuron's delta and its jth input</text>

<text 
x="171" 
y="1088" 
class="s8_307"
>output_neuron</text>

<text 
x="274" 
y="1088" 
class="s7_307"
>[</text>

<text 
x="282" 
y="1088" 
class="s8_307"
>j</text>

<text 
x="290" 
y="1088" 
dx="0,0,4.6,0" 
class="s7_307"
>] -=</text>

<text 
x="329" 
y="1088" 
class="s8_307"
>output_deltas</text>

<text 
x="432" 
y="1088" 
class="s7_307"
>[</text>

<text 
x="440" 
y="1088" 
class="s8_307"
>i</text>

<text 
x="448" 
y="1088" 
dx="0,0,4.6" 
class="s7_307"
>] *</text>

<text 
x="480" 
y="1088" 
class="s8_307"
>hidden_output</text>

<text 
x="108" 
y="1118" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s9_307"
># back-propagate errors to hidden layer</text>

<text 
x="108" 
y="1134" 
class="s8_307"
>hidden_deltas</text>

<text 
x="219" 
y="1134" 
dx="0,0,4.6" 
class="s7_307"
>= [</text>

<text 
x="242" 
y="1134" 
class="s8_307"
>hidden_output</text>

<text 
x="353" 
y="1134" 
dx="0,0,4.6" 
class="s7_307"
>* (</text>

<text 
x="377" 
y="1134" 
class="s10_307"
>1</text>

<text 
x="393" 
y="1134" 
class="s7_307"
>-</text>

<text 
x="408" 
y="1134" 
class="s8_307"
>hidden_output</text>

<text 
x="511" 
y="1134" 
dx="0,0,4.6" 
class="s7_307"
>) *</text>

<text 
x="250" 
y="1149" 
class="s8_307"
>dot</text>

<text 
x="274" 
y="1149" 
class="s7_307"
>(</text>

<text 
x="282" 
y="1149" 
class="s8_307"
>output_deltas</text>

<text 
x="385" 
y="1149" 
dx="0,0,4.6" 
class="s7_307"
>, [</text>

<text 
x="408" 
y="1149" 
class="s8_307"
>n</text>

<text 
x="416" 
y="1149" 
class="s7_307"
>[</text>

<text 
x="424" 
y="1149" 
class="s8_307"
>i</text>

<text 
x="432" 
y="1149" 
class="s7_307"
>]</text>

<text 
x="448" 
y="1149" 
class="s5_307"
>for</text>

<text 
x="480" 
y="1149" 
class="s8_307"
>n</text>

<text 
x="495" 
y="1149" 
class="s11_307"
>in</text>

<text 
x="519" 
y="1149" 
class="s8_307"
>output_layer</text>

<text 
x="614" 
y="1149" 
class="s7_307"
>])</text>

<text 
x="242" 
y="1165" 
class="s5_307"
>for</text>

<text 
x="274" 
y="1165" 
class="s8_307"
>i</text>

<text 
x="282" 
y="1165" 
class="s7_307"
>,</text>

<text 
x="298" 
y="1165" 
class="s8_307"
>hidden_output</text>

<text 
x="408" 
y="1165" 
class="s11_307"
>in</text>

<text 
x="432" 
y="1165" 
class="s12_307"
>enumerate</text>

<text 
x="503" 
y="1165" 
class="s7_307"
>(</text>

<text 
x="511" 
y="1165" 
class="s8_307"
>hidden_outputs</text>

<text 
x="622" 
y="1165" 
class="s7_307"
>)]</text>

<text 
x="108" 
y="1195" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,4.6,0,4.6,0,0,0" 
class="s9_307"
># adjust weights for hidden layer, one neuron at a time</text>

<text 
x="108" 
y="1211" 
class="s5_307"
>for</text>

<text 
x="140" 
y="1211" 
class="s8_307"
>i</text>

<text 
x="148" 
y="1211" 
class="s7_307"
>,</text>

<text 
x="163" 
y="1211" 
class="s8_307"
>hidden_neuron</text>

<text 
x="274" 
y="1211" 
class="s11_307"
>in</text>

<text 
x="298" 
y="1211" 
class="s12_307"
>enumerate</text>

<text 
x="369" 
y="1211" 
class="s7_307"
>(</text>

<text 
x="377" 
y="1211" 
class="s8_307"
>network</text>

<text 
x="432" 
y="1211" 
class="s7_307"
>[</text>

<text 
x="440" 
y="1211" 
class="s10_307"
>0</text>

<text 
x="448" 
y="1211" 
class="s7_307"
>]):</text>

<text 
x="140" 
y="1226" 
class="s5_307"
>for</text>

<text 
x="171" 
y="1226" 
class="s8_307"
>j</text>

<text 
x="179" 
y="1226" 
class="s7_307"
>,</text>

<text 
x="195" 
y="1226" 
class="s12_307"
>input</text>

<text 
x="242" 
y="1226" 
class="s11_307"
>in</text>

<text 
x="266" 
y="1226" 
class="s12_307"
>enumerate</text>

<text 
x="337" 
y="1226" 
class="s7_307"
>(</text>

<text 
x="345" 
y="1226" 
class="s8_307"
>input_vector</text>

<text 
x="448" 
y="1226" 
dx="0,0,4.6" 
class="s7_307"
>+ [</text>

<text 
x="472" 
y="1226" 
class="s10_307"
>1</text>

<text 
x="480" 
y="1226" 
class="s7_307"
>]):</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
