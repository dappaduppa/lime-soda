<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 288 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_288{
font-size: 16.50px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s2_288{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_288{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<image preserveAspectRatio="none" x="55" y="54" width="799" height="505" xlink:href="288/img/1.png" />
<text 
x="295" 
y="584" 
dx="0,0,0,0,0,-0.6,0,0,0,0,0,0,0,0,0,-0.3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.6,0" 
class="s1_288"
>Figure 17-1. A “guess the animal” decision tree</text>

<text 
x="55" 
y="626" 
class="s2_288"
>Decision trees have a lot to recommend them. They’re very easy to understand and</text>

<text 
x="55" 
y="653" 
class="s2_288"
>interpret, and the process by which they reach a prediction is completely transparent.</text>

<text 
x="55" 
y="681" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>Unlike the other models we’ve looked at so far, decision trees can easily handle a mix of</text>

<text 
x="55" 
y="708" 
class="s2_288"
>numeric (e.g., number of legs) and categorical (e.g., delicious/not delicious) attributes and</text>

<text 
x="55" 
y="736" 
class="s2_288"
>can even classify data for which attributes are missing.</text>

<text 
x="55" 
y="774" 
class="s2_288"
>At the same time, finding an “optimal” decision tree for a set of training data is</text>

<text 
x="55" 
y="802" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>computationally a very hard problem. (We will get around this by trying to build a good-</text>

<text 
x="55" 
y="829" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>enough tree rather than an optimal one, although for large data sets this can still be a lot of</text>

<text 
x="55" 
y="857" 
class="s2_288"
>work.) More important, it is very easy (and very bad) to build decision trees that are</text>

<text 
x="55" 
y="884" 
class="s3_288"
>overfitted</text>

<text 
x="144" 
y="884" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.8,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>to the training data, and that don’t generalize well to unseen data. We’ll look at</text>

<text 
x="55" 
y="912" 
class="s2_288"
>ways to address this.</text>

<text 
x="55" 
y="950" 
class="s2_288"
>Most people divide decision trees into</text>

<text 
x="395" 
y="950" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.8,0,0" 
class="s3_288"
>classification trees</text>

<text 
x="565" 
y="950" 
class="s2_288"
>(which produce categorical</text>

<text 
x="55" 
y="978" 
class="s2_288"
>outputs) and</text>

<text 
x="169" 
y="978" 
dx="0,-0.8,0,0,-0.8,0,0,0,0,0,0,0,0,-0.8,0,0" 
class="s3_288"
>regression trees</text>

<text 
x="313" 
y="978" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>(which produce numeric outputs). In this chapter, we’ll focus</text>

<text 
x="55" 
y="1005" 
class="s2_288"
>on classification trees, and we’ll work through the ID3 algorithm for learning a decision</text>

<text 
x="55" 
y="1033" 
class="s2_288"
>tree from a set of labeled data, which should help us understand how decision trees</text>

<text 
x="55" 
y="1060" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>actually work. To make things simple, we’ll restrict ourselves to problems with binary</text>

<text 
x="55" 
y="1088" 
class="s2_288"
>outputs like “should I hire this candidate?” or “should I show this website visitor</text>

<text 
x="55" 
y="1115" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0" 
class="s2_288"
>advertisement A or advertisement B?” or “will eating this food I found in the office fridge</text>

<text 
x="55" 
y="1143" 
class="s2_288"
>make me sick?”</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
