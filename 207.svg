<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 207 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_207{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_207{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_207{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_207{
font-size: 16.50px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s5_207{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #8E0012;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
class="s1_207"
>Overfitting and Underfitting</text>

<text 
x="55" 
y="122" 
dx="0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>A common danger in machine learning is</text>

<text 
x="423" 
y="122" 
class="s3_207"
>overfitting</text>

<text 
x="520" 
y="122" 
class="s2_207"
>— producing a model that performs</text>

<text 
x="55" 
y="149" 
class="s2_207"
>well on the data you train it on but that generalizes poorly to any new data. This could</text>

<text 
x="55" 
y="177" 
class="s2_207"
>involve learning</text>

<text 
x="203" 
y="177" 
class="s3_207"
>noise</text>

<text 
x="255" 
y="177" 
class="s2_207"
>in the data. Or it could involve learning to identify specific inputs</text>

<text 
x="55" 
y="204" 
class="s2_207"
>rather than whatever factors are actually predictive for the desired output.</text>

<text 
x="55" 
y="243" 
class="s2_207"
>The other side of this is</text>

<text 
x="267" 
y="243" 
class="s3_207"
>underfitting</text>

<text 
x="371" 
y="243" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>, producing a model that doesn’t perform well even on</text>

<text 
x="55" 
y="270" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0" 
class="s2_207"
>the training data, although typically when this happens you decide your model isn’t good</text>

<text 
x="55" 
y="298" 
class="s2_207"
>enough and keep looking for a better one.</text>

<image preserveAspectRatio="none" x="92" y="321" width="725" height="587" xlink:href="207/img/1.png" />
<text 
x="320" 
y="934" 
dx="0,0,0,0,0,-0.6,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s4_207"
>Figure 11-1. Overfitting and underfitting</text>

<text 
x="55" 
y="975" 
class="s2_207"
>In</text>

<text 
x="78" 
y="975" 
dx="0,0,0,0,0,0,0,0,-0.8,0,0" 
class="s5_207"
>Figure 11-1</text>

<text 
x="181" 
y="975" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>, I’ve fit three polynomials to a sample of data. (Don’t worry about how;</text>

<text 
x="55" 
y="1003" 
class="s2_207"
>we’ll get to that in later chapters.)</text>

<text 
x="55" 
y="1041" 
class="s2_207"
>The horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It severely</text>

<text 
x="55" 
y="1069" 
class="s3_207"
>underfits</text>

<text 
x="138" 
y="1069" 
class="s2_207"
>the training data. The best fit degree 9 (i.e., 10-parameter) polynomial goes</text>

<text 
x="55" 
y="1096" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>through every training data point exactly, but it very severely</text>

<text 
x="597" 
y="1096" 
class="s3_207"
>overfits</text>

<text 
x="668" 
y="1096" 
class="s2_207"
>— if we were to</text>

<text 
x="55" 
y="1124" 
class="s2_207"
>pick a few more data points it would quite likely miss them by a lot. And the degree 1 line</text>

<text 
x="55" 
y="1151" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>strikes a nice balance — it’s pretty close to every point, and (if these data are</text>

<text 
x="55" 
y="1179" 
class="s2_207"
>representative) the line will likely be close to new data points as well.</text>

<text 
x="55" 
y="1217" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_207"
>Clearly models that are too complex lead to overfitting and don’t generalize well beyond</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
