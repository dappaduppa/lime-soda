<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 152 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_152{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_152{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_152{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_152{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s5_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC00FF;
}
.s6_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s7_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s8_152{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #CC3300;
}
.s9_152{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}
.s10_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #336666;
}
.s11_152{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}
.s12_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}
.s13_152{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC3300;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
class="s1_152"
>Stochastic Gradient Descent</text>

<text 
x="55" 
y="122" 
class="s2_152"
>As we mentioned before, often we’ll be using gradient descent to choose the parameters of</text>

<text 
x="55" 
y="149" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_152"
>a model in a way that minimizes some notion of error. Using the previous batch approach,</text>

<text 
x="55" 
y="177" 
class="s2_152"
>each gradient step requires us to make a prediction and compute the gradient for the whole</text>

<text 
x="55" 
y="204" 
class="s2_152"
>data set, which makes each step take a long time.</text>

<text 
x="55" 
y="243" 
dx="0,0,0,-1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_152"
>Now, usually these error functions are</text>

<text 
x="394" 
y="243" 
class="s3_152"
>additive</text>

<text 
x="465" 
y="243" 
class="s2_152"
>, which means that the predictive error on</text>

<text 
x="55" 
y="270" 
class="s2_152"
>the whole data set is simply the sum of the predictive errors for each data point.</text>

<text 
x="55" 
y="309" 
class="s2_152"
>When this is the case, we can instead apply a technique called</text>

<text 
x="605" 
y="309" 
class="s3_152"
>stochastic gradient descent</text>

<text 
x="844" 
y="309" 
class="s2_152"
>,</text>

<text 
x="55" 
y="336" 
class="s2_152"
>which computes the gradient (and takes a step) for only one point at a time. It cycles over</text>

<text 
x="55" 
y="364" 
class="s2_152"
>our data repeatedly until it reaches a stopping point.</text>

<text 
x="55" 
y="402" 
class="s2_152"
>During each cycle, we’ll want to iterate through our data in a random order:</text>

<text 
x="76" 
y="447" 
class="s4_152"
>def</text>

<text 
x="108" 
y="447" 
class="s5_152"
>in_random_order</text>

<text 
x="227" 
y="447" 
class="s6_152"
>(</text>

<text 
x="235" 
y="447" 
class="s7_152"
>data</text>

<text 
x="266" 
y="447" 
class="s6_152"
>):</text>

<text 
x="108" 
y="463" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0" 
class="s8_152"
>"""generator that returns the elements of data in random order"""</text>

<text 
x="108" 
y="478" 
class="s7_152"
>indexes</text>

<text 
x="171" 
y="478" 
dx="0,0,4.6" 
class="s6_152"
>= [</text>

<text 
x="195" 
y="478" 
class="s7_152"
>i</text>

<text 
x="211" 
y="478" 
class="s4_152"
>for</text>

<text 
x="242" 
y="478" 
class="s7_152"
>i</text>

<text 
x="250" 
y="478" 
class="s6_152"
>,</text>

<text 
x="266" 
y="478" 
class="s7_152"
>_</text>

<text 
x="282" 
y="478" 
class="s9_152"
>in</text>

<text 
x="306" 
y="478" 
class="s10_152"
>enumerate</text>

<text 
x="377" 
y="478" 
class="s6_152"
>(</text>

<text 
x="385" 
y="478" 
class="s7_152"
>data</text>

<text 
x="416" 
y="478" 
class="s6_152"
>)]</text>

<text 
x="448" 
y="478" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0" 
class="s11_152"
># create a list of indexes</text>

<text 
x="108" 
y="494" 
class="s7_152"
>random</text>

<text 
x="156" 
y="494" 
class="s6_152"
>.</text>

<text 
x="163" 
y="494" 
class="s7_152"
>shuffle</text>

<text 
x="219" 
y="494" 
class="s6_152"
>(</text>

<text 
x="227" 
y="494" 
class="s7_152"
>indexes</text>

<text 
x="282" 
y="494" 
class="s6_152"
>)</text>

<text 
x="448" 
y="494" 
dx="0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0" 
class="s11_152"
># shuffle them</text>

<text 
x="108" 
y="509" 
class="s4_152"
>for</text>

<text 
x="140" 
y="509" 
class="s7_152"
>i</text>

<text 
x="156" 
y="509" 
class="s9_152"
>in</text>

<text 
x="179" 
y="509" 
class="s7_152"
>indexes</text>

<text 
x="235" 
y="509" 
class="s6_152"
>:</text>

<text 
x="448" 
y="509" 
dx="0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,0,0" 
class="s11_152"
># return the data in that order</text>

<text 
x="140" 
y="524" 
class="s4_152"
>yield</text>

<text 
x="187" 
y="524" 
class="s7_152"
>data</text>

<text 
x="219" 
y="524" 
class="s6_152"
>[</text>

<text 
x="227" 
y="524" 
class="s7_152"
>i</text>

<text 
x="235" 
y="524" 
class="s6_152"
>]</text>

<text 
x="55" 
y="577" 
class="s2_152"
>And we’ll want to take a gradient step for each data point. This approach leaves the</text>

<text 
x="55" 
y="605" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_152"
>possibility that we might circle around near a minimum forever, so whenever we stop</text>

<text 
x="55" 
y="632" 
class="s2_152"
>getting improvements we’ll decrease the step size and eventually quit:</text>

<text 
x="76" 
y="677" 
class="s4_152"
>def</text>

<text 
x="108" 
y="677" 
class="s5_152"
>minimize_stochastic</text>

<text 
x="258" 
y="677" 
class="s6_152"
>(</text>

<text 
x="266" 
y="677" 
class="s7_152"
>target_fn</text>

<text 
x="337" 
y="677" 
class="s6_152"
>,</text>

<text 
x="353" 
y="677" 
class="s7_152"
>gradient_fn</text>

<text 
x="440" 
y="677" 
class="s6_152"
>,</text>

<text 
x="456" 
y="677" 
class="s7_152"
>x</text>

<text 
x="464" 
y="677" 
class="s6_152"
>,</text>

<text 
x="480" 
y="677" 
class="s7_152"
>y</text>

<text 
x="487" 
y="677" 
class="s6_152"
>,</text>

<text 
x="503" 
y="677" 
class="s7_152"
>theta_0</text>

<text 
x="559" 
y="677" 
class="s6_152"
>,</text>

<text 
x="574" 
y="677" 
class="s7_152"
>alpha_0</text>

<text 
x="630" 
y="677" 
class="s6_152"
>=</text>

<text 
x="638" 
y="677" 
class="s12_152"
>0.01</text>

<text 
x="669" 
y="677" 
class="s6_152"
>):</text>

<text 
x="108" 
y="708" 
class="s7_152"
>data</text>

<text 
x="148" 
y="708" 
class="s6_152"
>=</text>

<text 
x="163" 
y="708" 
class="s10_152"
>zip</text>

<text 
x="187" 
y="708" 
class="s6_152"
>(</text>

<text 
x="195" 
y="708" 
class="s7_152"
>x</text>

<text 
x="203" 
y="708" 
class="s6_152"
>,</text>

<text 
x="219" 
y="708" 
class="s7_152"
>y</text>

<text 
x="227" 
y="708" 
class="s6_152"
>)</text>

<text 
x="108" 
y="723" 
class="s7_152"
>theta</text>

<text 
x="156" 
y="723" 
class="s6_152"
>=</text>

<text 
x="171" 
y="723" 
class="s7_152"
>theta_0</text>

<text 
x="456" 
y="723" 
dx="0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0" 
class="s11_152"
># initial guess</text>

<text 
x="108" 
y="739" 
class="s7_152"
>alpha</text>

<text 
x="156" 
y="739" 
class="s6_152"
>=</text>

<text 
x="171" 
y="739" 
class="s7_152"
>alpha_0</text>

<text 
x="456" 
y="739" 
dx="0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0" 
class="s11_152"
># initial step size</text>

<text 
x="108" 
y="754" 
class="s7_152"
>min_theta</text>

<text 
x="179" 
y="754" 
class="s6_152"
>,</text>

<text 
x="195" 
y="754" 
class="s7_152"
>min_value</text>

<text 
x="274" 
y="754" 
class="s6_152"
>=</text>

<text 
x="290" 
y="754" 
class="s10_152"
>None</text>

<text 
x="322" 
y="754" 
class="s6_152"
>,</text>

<text 
x="337" 
y="754" 
class="s10_152"
>float</text>

<text 
x="377" 
y="754" 
class="s6_152"
>(</text>

<text 
x="385" 
y="754" 
class="s13_152"
>"inf"</text>

<text 
x="424" 
y="754" 
class="s6_152"
>)</text>

<text 
x="456" 
y="754" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0" 
class="s11_152"
># the minimum so far</text>

<text 
x="108" 
y="770" 
class="s7_152"
>iterations_with_no_improvement</text>

<text 
x="353" 
y="770" 
class="s6_152"
>=</text>

<text 
x="369" 
y="770" 
class="s12_152"
>0</text>

<text 
x="108" 
y="800" 
dx="0,0,4.6,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0" 
class="s11_152"
># if we ever go 100 iterations with no improvement, stop</text>

<text 
x="108" 
y="816" 
class="s4_152"
>while</text>

<text 
x="156" 
y="816" 
class="s7_152"
>iterations_with_no_improvement</text>

<text 
x="401" 
y="816" 
class="s6_152"
>&lt;</text>

<text 
x="416" 
y="816" 
class="s12_152"
>100</text>

<text 
x="440" 
y="816" 
class="s6_152"
>:</text>

<text 
x="140" 
y="831" 
class="s7_152"
>value</text>

<text 
x="187" 
y="831" 
class="s6_152"
>=</text>

<text 
x="203" 
y="831" 
class="s10_152"
>sum</text>

<text 
x="227" 
y="831" 
class="s6_152"
>(</text>

<text 
x="242" 
y="831" 
class="s7_152"
>target_fn</text>

<text 
x="314" 
y="831" 
class="s6_152"
>(</text>

<text 
x="322" 
y="831" 
class="s7_152"
>x_i</text>

<text 
x="345" 
y="831" 
class="s6_152"
>,</text>

<text 
x="361" 
y="831" 
class="s7_152"
>y_i</text>

<text 
x="385" 
y="831" 
class="s6_152"
>,</text>

<text 
x="401" 
y="831" 
class="s7_152"
>theta</text>

<text 
x="440" 
y="831" 
class="s6_152"
>)</text>

<text 
x="456" 
y="831" 
class="s4_152"
>for</text>

<text 
x="487" 
y="831" 
class="s7_152"
>x_i</text>

<text 
x="511" 
y="831" 
class="s6_152"
>,</text>

<text 
x="527" 
y="831" 
class="s7_152"
>y_i</text>

<text 
x="559" 
y="831" 
class="s9_152"
>in</text>

<text 
x="582" 
y="831" 
class="s7_152"
>data</text>

<text 
x="622" 
y="831" 
class="s6_152"
>)</text>

<text 
x="140" 
y="862" 
class="s4_152"
>if</text>

<text 
x="163" 
y="862" 
class="s7_152"
>value</text>

<text 
x="211" 
y="862" 
class="s6_152"
>&lt;</text>

<text 
x="227" 
y="862" 
class="s7_152"
>min_value</text>

<text 
x="298" 
y="862" 
class="s6_152"
>:</text>

<text 
x="171" 
y="878" 
dx="0,0,4.6,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0" 
class="s11_152"
># if we've found a new minimum, remember it</text>

<text 
x="171" 
y="893" 
dx="0,0,4.6,0,0,0,4.6,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0" 
class="s11_152"
># and go back to the original step size</text>

<text 
x="171" 
y="908" 
class="s7_152"
>min_theta</text>

<text 
x="242" 
y="908" 
class="s6_152"
>,</text>

<text 
x="258" 
y="908" 
class="s7_152"
>min_value</text>

<text 
x="337" 
y="908" 
class="s6_152"
>=</text>

<text 
x="353" 
y="908" 
class="s7_152"
>theta</text>

<text 
x="393" 
y="908" 
class="s6_152"
>,</text>

<text 
x="408" 
y="908" 
class="s7_152"
>value</text>

<text 
x="171" 
y="924" 
class="s7_152"
>iterations_with_no_improvement</text>

<text 
x="416" 
y="924" 
class="s6_152"
>=</text>

<text 
x="432" 
y="924" 
class="s12_152"
>0</text>

<text 
x="171" 
y="939" 
class="s7_152"
>alpha</text>

<text 
x="219" 
y="939" 
class="s6_152"
>=</text>

<text 
x="235" 
y="939" 
class="s7_152"
>alpha_0</text>

<text 
x="140" 
y="955" 
class="s4_152"
>else</text>

<text 
x="171" 
y="955" 
class="s6_152"
>:</text>

<text 
x="171" 
y="970" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0" 
class="s11_152"
># otherwise we're not improving, so try shrinking the step size</text>

<text 
x="171" 
y="985" 
class="s7_152"
>iterations_with_no_improvement</text>

<text 
x="416" 
y="985" 
class="s6_152"
>+=</text>

<text 
x="440" 
y="985" 
class="s12_152"
>1</text>

<text 
x="171" 
y="1001" 
class="s7_152"
>alpha</text>

<text 
x="219" 
y="1001" 
class="s6_152"
>*=</text>

<text 
x="242" 
y="1001" 
class="s12_152"
>0.9</text>

<text 
x="140" 
y="1032" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,4.6,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0" 
class="s11_152"
># and take a gradient step for each of the data points</text>

<text 
x="140" 
y="1047" 
class="s4_152"
>for</text>

<text 
x="171" 
y="1047" 
class="s7_152"
>x_i</text>

<text 
x="195" 
y="1047" 
class="s6_152"
>,</text>

<text 
x="211" 
y="1047" 
class="s7_152"
>y_i</text>

<text 
x="242" 
y="1047" 
class="s9_152"
>in</text>

<text 
x="266" 
y="1047" 
class="s7_152"
>in_random_order</text>

<text 
x="385" 
y="1047" 
class="s6_152"
>(</text>

<text 
x="393" 
y="1047" 
class="s7_152"
>data</text>

<text 
x="424" 
y="1047" 
class="s6_152"
>):</text>

<text 
x="171" 
y="1062" 
class="s7_152"
>gradient_i</text>

<text 
x="258" 
y="1062" 
class="s6_152"
>=</text>

<text 
x="274" 
y="1062" 
class="s7_152"
>gradient_fn</text>

<text 
x="361" 
y="1062" 
class="s6_152"
>(</text>

<text 
x="369" 
y="1062" 
class="s7_152"
>x_i</text>

<text 
x="393" 
y="1062" 
class="s6_152"
>,</text>

<text 
x="408" 
y="1062" 
class="s7_152"
>y_i</text>

<text 
x="432" 
y="1062" 
class="s6_152"
>,</text>

<text 
x="448" 
y="1062" 
class="s7_152"
>theta</text>

<text 
x="487" 
y="1062" 
class="s6_152"
>)</text>

<text 
x="171" 
y="1078" 
class="s7_152"
>theta</text>

<text 
x="219" 
y="1078" 
class="s6_152"
>=</text>

<text 
x="235" 
y="1078" 
class="s7_152"
>vector_subtract</text>

<text 
x="353" 
y="1078" 
class="s6_152"
>(</text>

<text 
x="361" 
y="1078" 
class="s7_152"
>theta</text>

<text 
x="401" 
y="1078" 
class="s6_152"
>,</text>

<text 
x="416" 
y="1078" 
class="s7_152"
>scalar_multiply</text>

<text 
x="535" 
y="1078" 
class="s6_152"
>(</text>

<text 
x="543" 
y="1078" 
class="s7_152"
>alpha</text>

<text 
x="582" 
y="1078" 
class="s6_152"
>,</text>

<text 
x="598" 
y="1078" 
class="s7_152"
>gradient_i</text>

<text 
x="677" 
y="1078" 
class="s6_152"
>))</text>

<text 
x="108" 
y="1109" 
class="s4_152"
>return</text>

<text 
x="163" 
y="1109" 
class="s7_152"
>min_theta</text>

<text 
x="55" 
y="1161" 
class="s2_152"
>The stochastic version will typically be a lot faster than the batch version. Of course, we’ll</text>

<text 
x="55" 
y="1189" 
class="s2_152"
>want a version that maximizes as well:</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
