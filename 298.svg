<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">


<!-- Page 298 -->
<svg x="0" y="0" width="909" height="1286" viewBox="0 0 909 1286" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
style="display: block;margin-left: auto;margin-right: auto;">
<defs>

<style type="text/css"><![CDATA[


.s1_298{
font-size: 30.81px;
font-family: LiberationSerif-Bold_b;
fill: #8E0012;
}
.s2_298{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #000000;
}
.s3_298{
font-size: 22.01px;
font-family: LiberationSerif-Italic_l;
fill: #000000;
}
.s4_298{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #006699;
}
.s5_298{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #CC00FF;
}
.s6_298{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s7_298{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #000088;
}
.s8_298{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #336666;
}
.s9_298{
font-size: 13.20px;
font-family: LiberationMono-Bold_1w;
fill: #000000;
}
.s10_298{
font-size: 13.20px;
font-family: LiberationMono_1q;
fill: #FF6600;
}
.s11_298{
font-size: 22.01px;
font-family: LiberationSerif_e;
fill: #8E0012;
}
.s12_298{
font-size: 17.60px;
font-family: LiberationMono_1q;
fill: #000000;
}
.s13_298{
font-size: 13.20px;
font-family: LiberationMono-Italic_10;
fill: #0099FF;
}

]]></style>

</defs>
<path d="M0,0
L0,1286
L909,1286
L909,0 Z " 
fill="#FFFFFF" stroke="none" />
<text 
x="55" 
y="81" 
dx="0,0,0,0,0,0,0,0,0,0,-0.6,0,0,0" 
class="s1_298"
>Random Forests</text>

<text 
x="55" 
y="122" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0" 
class="s2_298"
>Given how closely decision trees can fit themselves to their training data, it’s not</text>

<text 
x="55" 
y="149" 
class="s2_298"
>surprising that they have a tendency to overfit. One way of avoiding this is a technique</text>

<text 
x="55" 
y="177" 
class="s2_298"
>called</text>

<text 
x="112" 
y="177" 
dx="0,0,0,0,0,0,0,0,0,0,-0.8,0,0,0" 
class="s3_298"
>random forests</text>

<text 
x="244" 
y="177" 
class="s2_298"
>, in which we build multiple decision trees and let them vote on how</text>

<text 
x="55" 
y="204" 
class="s2_298"
>to classify inputs:</text>

<text 
x="76" 
y="249" 
class="s4_298"
>def</text>

<text 
x="108" 
y="249" 
class="s5_298"
>forest_classify</text>

<text 
x="227" 
y="249" 
class="s6_298"
>(</text>

<text 
x="235" 
y="249" 
class="s7_298"
>trees</text>

<text 
x="274" 
y="249" 
class="s6_298"
>,</text>

<text 
x="290" 
y="249" 
class="s8_298"
>input</text>

<text 
x="329" 
y="249" 
class="s6_298"
>):</text>

<text 
x="108" 
y="265" 
class="s7_298"
>votes</text>

<text 
x="156" 
y="265" 
dx="0,0,4.6" 
class="s6_298"
>= [</text>

<text 
x="179" 
y="265" 
class="s7_298"
>classify</text>

<text 
x="242" 
y="265" 
class="s6_298"
>(</text>

<text 
x="250" 
y="265" 
class="s7_298"
>tree</text>

<text 
x="282" 
y="265" 
class="s6_298"
>,</text>

<text 
x="298" 
y="265" 
class="s8_298"
>input</text>

<text 
x="337" 
y="265" 
class="s6_298"
>)</text>

<text 
x="353" 
y="265" 
class="s4_298"
>for</text>

<text 
x="385" 
y="265" 
class="s7_298"
>tree</text>

<text 
x="424" 
y="265" 
class="s9_298"
>in</text>

<text 
x="448" 
y="265" 
class="s7_298"
>trees</text>

<text 
x="487" 
y="265" 
class="s6_298"
>]</text>

<text 
x="108" 
y="280" 
class="s7_298"
>vote_counts</text>

<text 
x="203" 
y="280" 
class="s6_298"
>=</text>

<text 
x="219" 
y="280" 
class="s7_298"
>Counter</text>

<text 
x="274" 
y="280" 
class="s6_298"
>(</text>

<text 
x="282" 
y="280" 
class="s7_298"
>votes</text>

<text 
x="322" 
y="280" 
class="s6_298"
>)</text>

<text 
x="108" 
y="295" 
class="s4_298"
>return</text>

<text 
x="163" 
y="295" 
class="s7_298"
>vote_counts</text>

<text 
x="250" 
y="295" 
class="s6_298"
>.</text>

<text 
x="258" 
y="295" 
class="s7_298"
>most_common</text>

<text 
x="345" 
y="295" 
class="s6_298"
>(</text>

<text 
x="353" 
y="295" 
class="s10_298"
>1</text>

<text 
x="361" 
y="295" 
class="s6_298"
>)[</text>

<text 
x="377" 
y="295" 
class="s10_298"
>0</text>

<text 
x="385" 
y="295" 
class="s6_298"
>][</text>

<text 
x="401" 
y="295" 
class="s10_298"
>0</text>

<text 
x="408" 
y="295" 
class="s6_298"
>]</text>

<text 
x="55" 
y="348" 
class="s2_298"
>Our tree-building process was deterministic, so how do we get random trees?</text>

<text 
x="55" 
y="387" 
class="s2_298"
>One piece involves bootstrapping data (recall</text>

<text 
x="459" 
y="387" 
class="s11_298"
>“Digression: The Bootstrap”</text>

<text 
x="710" 
y="387" 
class="s2_298"
>). Rather than</text>

<text 
x="55" 
y="414" 
class="s2_298"
>training each tree on all the</text>

<text 
x="300" 
y="414" 
class="s12_298"
>inputs</text>

<text 
x="369" 
y="414" 
class="s2_298"
>in the training set, we train each tree on the result of</text>

<text 
x="55" 
y="444" 
class="s12_298"
>bootstrap_sample(inputs)</text>

<text 
x="308" 
y="444" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_298"
>. Since each tree is built using different data, each tree will be</text>

<text 
x="55" 
y="474" 
dx="0,0,0,-0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_298"
>different from every other tree. (A side benefit is that it’s totally fair to use the nonsampled</text>

<text 
x="55" 
y="501" 
class="s2_298"
>data to test each tree, which means you can get away with using all of your data as the</text>

<text 
x="55" 
y="529" 
class="s2_298"
>training set if you are clever in how you measure performance.) This technique is known</text>

<text 
x="55" 
y="556" 
class="s2_298"
>as</text>

<text 
x="78" 
y="556" 
dx="0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.8,0,0,0,0,0,0" 
class="s3_298"
>bootstrap aggregating</text>

<text 
x="280" 
y="556" 
class="s2_298"
>or</text>

<text 
x="304" 
y="556" 
class="s3_298"
>bagging</text>

<text 
x="376" 
y="556" 
class="s2_298"
>.</text>

<text 
x="55" 
y="595" 
dx="0,0,-1.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0" 
class="s2_298"
>A second source of randomness involves changing the way we chose the</text>

<text 
x="698" 
y="595" 
class="s12_298"
>best_attribute</text>

<text 
x="55" 
y="624" 
class="s2_298"
>to split on. Rather than looking at all the remaining attributes, we first choose a random</text>

<text 
x="55" 
y="652" 
class="s2_298"
>subset of them and then split on whichever of those is best:</text>

<text 
x="108" 
y="697" 
dx="0,0,4.6,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,4.6,0,0,0,4.6,0,0,4.6,0,0,0" 
class="s13_298"
># if there's already few enough split candidates, look at all of them</text>

<text 
x="108" 
y="712" 
class="s4_298"
>if</text>

<text 
x="132" 
y="712" 
class="s8_298"
>len</text>

<text 
x="156" 
y="712" 
class="s6_298"
>(</text>

<text 
x="163" 
y="712" 
class="s7_298"
>split_candidates</text>

<text 
x="290" 
y="712" 
dx="0,0,4.6,0" 
class="s6_298"
>) &lt;=</text>

<text 
x="329" 
y="712" 
class="s8_298"
>self</text>

<text 
x="361" 
y="712" 
class="s6_298"
>.</text>

<text 
x="369" 
y="712" 
class="s7_298"
>num_split_candidates</text>

<text 
x="527" 
y="712" 
class="s6_298"
>:</text>

<text 
x="140" 
y="728" 
class="s7_298"
>sampled_split_candidates</text>

<text 
x="337" 
y="728" 
class="s6_298"
>=</text>

<text 
x="353" 
y="728" 
class="s7_298"
>split_candidates</text>

<text 
x="108" 
y="743" 
dx="0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,4.6,0,0,0,0,0,0,4.6,0,0,0,0,0" 
class="s13_298"
># otherwise pick a random sample</text>

<text 
x="108" 
y="759" 
class="s4_298"
>else</text>

<text 
x="140" 
y="759" 
class="s6_298"
>:</text>

<text 
x="140" 
y="774" 
class="s7_298"
>sampled_split_candidates</text>

<text 
x="337" 
y="774" 
class="s6_298"
>=</text>

<text 
x="353" 
y="774" 
class="s7_298"
>random</text>

<text 
x="401" 
y="774" 
class="s6_298"
>.</text>

<text 
x="408" 
y="774" 
class="s7_298"
>sample</text>

<text 
x="456" 
y="774" 
class="s6_298"
>(</text>

<text 
x="464" 
y="774" 
class="s7_298"
>split_candidates</text>

<text 
x="590" 
y="774" 
class="s6_298"
>,</text>

<text 
x="464" 
y="789" 
class="s8_298"
>self</text>

<text 
x="495" 
y="789" 
class="s6_298"
>.</text>

<text 
x="503" 
y="789" 
class="s7_298"
>num_split_candidates</text>

<text 
x="661" 
y="789" 
class="s6_298"
>)</text>

<text 
x="108" 
y="820" 
dx="0,0,4.6,0,0,0,4.6,0,0,0,0,0,0,4.6,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,4.6,0,0,0,0,0,4.6,0,0,0,0,0,0,0,0,0" 
class="s13_298"
># now choose the best attribute only from those candidates</text>

<text 
x="108" 
y="836" 
class="s7_298"
>best_attribute</text>

<text 
x="227" 
y="836" 
class="s6_298"
>=</text>

<text 
x="242" 
y="836" 
class="s8_298"
>min</text>

<text 
x="266" 
y="836" 
class="s6_298"
>(</text>

<text 
x="274" 
y="836" 
class="s7_298"
>sampled_split_candidates</text>

<text 
x="464" 
y="836" 
class="s6_298"
>,</text>

<text 
x="140" 
y="851" 
class="s7_298"
>key</text>

<text 
x="163" 
y="851" 
class="s6_298"
>=</text>

<text 
x="171" 
y="851" 
class="s7_298"
>partial</text>

<text 
x="227" 
y="851" 
class="s6_298"
>(</text>

<text 
x="235" 
y="851" 
class="s7_298"
>partition_entropy_by</text>

<text 
x="393" 
y="851" 
class="s6_298"
>,</text>

<text 
x="408" 
y="851" 
class="s7_298"
>inputs</text>

<text 
x="456" 
y="851" 
class="s6_298"
>))</text>

<text 
x="108" 
y="882" 
class="s7_298"
>partitions</text>

<text 
x="195" 
y="882" 
class="s6_298"
>=</text>

<text 
x="211" 
y="882" 
class="s7_298"
>partition_by</text>

<text 
x="306" 
y="882" 
class="s6_298"
>(</text>

<text 
x="314" 
y="882" 
class="s7_298"
>inputs</text>

<text 
x="361" 
y="882" 
class="s6_298"
>,</text>

<text 
x="377" 
y="882" 
class="s7_298"
>best_attribute</text>

<text 
x="487" 
y="882" 
class="s6_298"
>)</text>

<text 
x="55" 
y="935" 
class="s2_298"
>This is an example of a broader technique called</text>

<text 
x="486" 
y="935" 
class="s3_298"
>ensemble learning</text>

<text 
x="653" 
y="935" 
class="s2_298"
>in which we combine</text>

<text 
x="55" 
y="962" 
class="s2_298"
>several</text>

<text 
x="122" 
y="962" 
class="s3_298"
>weak learners</text>

<text 
x="252" 
y="962" 
class="s2_298"
>(typically high-bias, low-variance models) in order to produce an</text>

<text 
x="55" 
y="990" 
class="s2_298"
>overall strong model.</text>

<text 
x="55" 
y="1028" 
class="s2_298"
>Random forests are one of the most popular and versatile models around.</text>



<!-- Any embedded fonts defined here -->
<style type="text/css" ><![CDATA[

@font-face {
	font-family: LiberationSerif-Italic_l;
	src: url("fonts/LiberationSerif-Italic_l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Bold_1w;
	src: url("fonts/LiberationMono-Bold_1w.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_1q;
	src: url("fonts/LiberationMono_1q.woff") format("woff");
}

@font-face {
	font-family: LiberationMono-Italic_10;
	src: url("fonts/LiberationMono-Italic_10.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif-Bold_b;
	src: url("fonts/LiberationSerif-Bold_b.woff") format("woff");
}

@font-face {
	font-family: LiberationSerif_e;
	src: url("fonts/LiberationSerif_e.woff") format("woff");
}

]]></style>

</svg>
